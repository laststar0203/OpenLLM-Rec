{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easydict\n",
    "\n",
    "args = easydict.EasyDict()\n",
    "\n",
    "args.dataset_code = 'ml-1m'\n",
    "args.min_rating = 0 if args.dataset_code == 'ml-1m' else 4\n",
    "\n",
    "args.min_uc = 5\n",
    "args.min_sc = 0\n",
    "args.split = 'leave_one_out'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATASET_ROOT_FOLDER = \"/home/laststar/data/model/Bert4Rec/dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from abc import *\n",
    "from pathlib import Path\n",
    "import os\n",
    "import tempfile\n",
    "import shutil\n",
    "import pickle\n",
    "\n",
    "import wget\n",
    "\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "import sys\n",
    "\n",
    "##### Util #####\n",
    "def download(url, savepath):\n",
    "    wget.download(url, str(savepath))\n",
    "\n",
    "\n",
    "def unzip(zippath, savepath):\n",
    "    zip = zipfile.ZipFile(zippath)\n",
    "    zip.extractall(savepath)\n",
    "    zip.close()\n",
    "\n",
    "\n",
    "def get_count(tp, id):\n",
    "    groups = tp[[id]].groupby(id, as_index=False)\n",
    "    count = groups.size()\n",
    "    return count\n",
    "\n",
    "\n",
    "def filter_triplets(tp, min_uc=5, min_sc=0):\n",
    "    # Only keep the triplets for items which were clicked on by at least min_sc users.\n",
    "    if min_sc > 0:\n",
    "        itemcount = get_count(tp, 'movieId')\n",
    "        tp = tp[tp['movieId'].isin(itemcount.index[itemcount >= min_sc])]\n",
    "\n",
    "    # Only keep the triplets for users who clicked on at least min_uc items\n",
    "    # After doing this, some of the items will have less than min_uc users, but should only be a small proportion\n",
    "    if min_uc > 0:\n",
    "        usercount = get_count(tp, 'userId')\n",
    "        tp = tp[tp['userId'].isin(usercount.index[usercount >= min_uc])]\n",
    "\n",
    "    # Update both usercount and itemcount after filtering\n",
    "    usercount, itemcount = get_count(tp, 'userId'), get_count(tp, 'movieId')\n",
    "    return tp, usercount, itemcount\n",
    "\n",
    "##### DATASET #####\n",
    "class AbstractDataset(metaclass=ABCMeta):\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.min_rating = args.min_rating\n",
    "        self.min_uc = args.min_uc\n",
    "        self.min_sc = args.min_sc\n",
    "        self.split = args.split\n",
    "\n",
    "        assert self.min_uc >= 2, 'Need at least 2 ratings per user for validation and test'\n",
    "\n",
    "    @classmethod\n",
    "    @abstractmethod\n",
    "    def code(cls):\n",
    "        pass\n",
    "\n",
    "    @classmethod\n",
    "    def raw_code(cls):\n",
    "        return cls.code()\n",
    "\n",
    "    @classmethod\n",
    "    @abstractmethod\n",
    "    def url(cls):\n",
    "        pass\n",
    "\n",
    "    @classmethod\n",
    "    def is_zipfile(cls):\n",
    "        return True\n",
    "\n",
    "    @classmethod\n",
    "    def zip_file_content_is_folder(cls):\n",
    "        return True\n",
    "\n",
    "    @classmethod\n",
    "    def all_raw_file_names(cls):\n",
    "        return []\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_ratings_df(self):\n",
    "        pass\n",
    "\n",
    "    def load_dataset(self):\n",
    "        self.preprocess()\n",
    "        dataset_path = self._get_preprocessed_dataset_path()\n",
    "        dataset = pickle.load(dataset_path.open('rb'))\n",
    "        return dataset\n",
    "\n",
    "    def preprocess(self):\n",
    "        dataset_path = self._get_preprocessed_dataset_path()\n",
    "        if dataset_path.is_file():\n",
    "            print('Already preprocessed. Skip preprocessing')\n",
    "            return\n",
    "        if not dataset_path.parent.is_dir():\n",
    "            dataset_path.parent.mkdir(parents=True)\n",
    "        self.maybe_download_raw_dataset()\n",
    "        df = self.load_ratings_df()\n",
    "        df = self.make_implicit(df)\n",
    "        df = self.filter_triplets(df)\n",
    "        df, umap, smap = self.densify_index(df)\n",
    "        train, val, test = self.split_df(df, len(umap))\n",
    "        dataset = {'train': train,\n",
    "                   'val': val,\n",
    "                   'test': test,\n",
    "                   'umap': umap,\n",
    "                   'smap': smap}\n",
    "        with dataset_path.open('wb') as f:\n",
    "            pickle.dump(dataset, f)\n",
    "\n",
    "    def maybe_download_raw_dataset(self):\n",
    "        folder_path = self._get_rawdata_folder_path()\n",
    "        if folder_path.is_dir() and\\\n",
    "           all(folder_path.joinpath(filename).is_file() for filename in self.all_raw_file_names()):\n",
    "            print('Raw data already exists. Skip downloading')\n",
    "            return\n",
    "        print(\"Raw file doesn't exist. Downloading...\")\n",
    "        if self.is_zipfile():\n",
    "            tmproot = Path(tempfile.mkdtemp())\n",
    "            tmpzip = tmproot.joinpath('file.zip')\n",
    "            tmpfolder = tmproot.joinpath('folder')\n",
    "            download(self.url(), tmpzip)\n",
    "            unzip(tmpzip, tmpfolder)\n",
    "            if self.zip_file_content_is_folder():\n",
    "                tmpfolder = tmpfolder.joinpath(os.listdir(tmpfolder)[0])\n",
    "            shutil.move(tmpfolder, folder_path)\n",
    "            shutil.rmtree(tmproot)\n",
    "            print()\n",
    "        else:\n",
    "            tmproot = Path(tempfile.mkdtemp())\n",
    "            tmpfile = tmproot.joinpath('file')\n",
    "            download(self.url(), tmpfile)\n",
    "            folder_path.mkdir(parents=True)\n",
    "            shutil.move(tmpfile, folder_path.joinpath('ratings.csv'))\n",
    "            shutil.rmtree(tmproot)\n",
    "            print()\n",
    "\n",
    "    def make_implicit(self, df):\n",
    "        print('Turning into implicit ratings')\n",
    "        df = df[df['rating'] >= self.min_rating]\n",
    "        # return df[['uid', 'sid', 'timestamp']]\n",
    "        return df\n",
    "\n",
    "    def filter_triplets(self, df):\n",
    "        print('Filtering triplets')\n",
    "        if self.min_sc > 0:\n",
    "            item_sizes = df.groupby('sid').size()\n",
    "            good_items = item_sizes.index[item_sizes >= self.min_sc]\n",
    "            df = df[df['sid'].isin(good_items)]\n",
    "\n",
    "        if self.min_uc > 0:\n",
    "            user_sizes = df.groupby('uid').size()\n",
    "            good_users = user_sizes.index[user_sizes >= self.min_uc]\n",
    "            df = df[df['uid'].isin(good_users)]\n",
    "\n",
    "        return df\n",
    "\n",
    "    def densify_index(self, df):\n",
    "        print('Densifying index')\n",
    "        umap = {u: i for i, u in enumerate(set(df['uid']))}\n",
    "        smap = {s: i for i, s in enumerate(set(df['sid']))}\n",
    "        df['uid'] = df['uid'].map(umap)\n",
    "        df['sid'] = df['sid'].map(smap)\n",
    "        return df, umap, smap\n",
    "\n",
    "    def split_df(self, df, user_count):\n",
    "        if self.args.split == 'leave_one_out':\n",
    "            print('Splitting')\n",
    "            user_group = df.groupby('uid')\n",
    "            user2items = user_group.progress_apply(lambda d: list(d.sort_values(by='timestamp')['sid']))\n",
    "            train, val, test = {}, {}, {}\n",
    "            for user in range(user_count):\n",
    "                items = user2items[user]\n",
    "                train[user], val[user], test[user] = items[:-2], items[-2:-1], items[-1:]\n",
    "            return train, val, test\n",
    "        elif self.args.split == 'holdout':\n",
    "            print('Splitting')\n",
    "            np.random.seed(self.args.dataset_split_seed)\n",
    "            eval_set_size = self.args.eval_set_size\n",
    "\n",
    "            # Generate user indices\n",
    "            permuted_index = np.random.permutation(user_count)\n",
    "            train_user_index = permuted_index[                :-2*eval_set_size]\n",
    "            val_user_index   = permuted_index[-2*eval_set_size:  -eval_set_size]\n",
    "            test_user_index  = permuted_index[  -eval_set_size:                ]\n",
    "\n",
    "            # Split DataFrames\n",
    "            train_df = df.loc[df['uid'].isin(train_user_index)]\n",
    "            val_df   = df.loc[df['uid'].isin(val_user_index)]\n",
    "            test_df  = df.loc[df['uid'].isin(test_user_index)]\n",
    "\n",
    "            # DataFrame to dict => {uid : list of sid's}\n",
    "            train = dict(train_df.groupby('uid').progress_apply(lambda d: list(d['sid'])))\n",
    "            val   = dict(val_df.groupby('uid').progress_apply(lambda d: list(d['sid'])))\n",
    "            test  = dict(test_df.groupby('uid').progress_apply(lambda d: list(d['sid'])))\n",
    "            return train, val, test\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def _get_rawdata_root_path(self):\n",
    "        return Path(RAW_DATASET_ROOT_FOLDER)\n",
    "\n",
    "    def _get_rawdata_folder_path(self):\n",
    "        root = self._get_rawdata_root_path()\n",
    "        return root.joinpath(self.raw_code())\n",
    "\n",
    "    def _get_preprocessed_root_path(self):\n",
    "        root = self._get_rawdata_root_path()\n",
    "        return root.joinpath('preprocessed')\n",
    "\n",
    "    def _get_preprocessed_folder_path(self):\n",
    "        preprocessed_root = self._get_preprocessed_root_path()\n",
    "        folder_name = '{}_min_rating{}-min_uc{}-min_sc{}-split{}' \\\n",
    "            .format(self.code(), self.min_rating, self.min_uc, self.min_sc, self.split)\n",
    "        return preprocessed_root.joinpath(folder_name)\n",
    "\n",
    "    def _get_preprocessed_dataset_path(self):\n",
    "        folder = self._get_preprocessed_folder_path()\n",
    "        return folder.joinpath('dataset.pkl')\n",
    "\n",
    "\n",
    "\n",
    "class ML1MDataset(AbstractDataset):\n",
    "    @classmethod\n",
    "    def code(cls):\n",
    "        return 'ml-1m'\n",
    "\n",
    "    @classmethod\n",
    "    def url(cls):\n",
    "        return 'http://files.grouplens.org/datasets/movielens/ml-1m.zip'\n",
    "\n",
    "    @classmethod\n",
    "    def zip_file_content_is_folder(cls):\n",
    "        return True\n",
    "\n",
    "    @classmethod\n",
    "    def all_raw_file_names(cls):\n",
    "        return ['README',\n",
    "                'movies.dat',\n",
    "                'ratings.dat',\n",
    "                'users.dat']\n",
    "\n",
    "    def load_ratings_df(self):\n",
    "        folder_path = self._get_rawdata_folder_path()\n",
    "        file_path = folder_path.joinpath('ratings.dat')\n",
    "        df = pd.read_csv(file_path, sep='::', header=None)\n",
    "        df.columns = ['uid', 'sid', 'rating', 'timestamp']\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already preprocessed. Skip preprocessing\n"
     ]
    }
   ],
   "source": [
    "dataset = ML1MDataset(args)\n",
    "dataset = dataset.load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset['train']\n",
    "val = dataset['val']\n",
    "test = dataset['test']\n",
    "umap = dataset['umap']\n",
    "smap = dataset['smap']\n",
    "user_count = len(umap)\n",
    "item_count = len(smap)\n",
    "\n",
    "args.num_items = len(smap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : '[2969, 1574, 957, 1178, 2147, 1658, 3177, 1117, 2599, 689, 1104, 253, 858, 593, 2488, 1781, 1848, 2889, 877, 1782, 970, 144, 1838, 963, 1025, 853, 2592, 1195, 2557, 1154, 639, 2710, 517, 2898, 2586, 964, 2128, 1107, 580, 2205, 1421, 513, 0, 574, 2483, 708, 581, 2102, 740, 2162, 1727]'\n",
      "1 : '[1108, 1127, 1120, 2512, 1201, 2735, 1135, 1104, 309, 2816, 2651, 1123, 1765, 1117, 579, 2879, 501, 1693, 3235, 1018, 2307, 1886, 106, 2821, 2931, 1155, 2889, 1259, 1106, 1777, 859, 1773, 1012, 1782, 1656, 3493, 3412, 3238, 1618, 1167, 1774, 2523, 1788, 841, 1031, 3219, 3107, 2645, 3341, 2853, 258, 2120, 576, 2856, 1161, 1152, 3457, 1153, 1775, 2046, 3436, 920, 1337, 2078, 2013, 3031, 228, 626, 1024, 484, 1154, 1047, 3647, 1414, 1099, 2203, 2128, 2166, 346, 3566, 2892, 1173, 2374, 443, 1848, 575, 370, 3186, 157, 1478, 2708, 466, 3032, 1406, 339, 1306, 20, 2160, 1826, 2086, 1271, 2234, 627, 1273, 1428, 1623, 2296, 1286, 737, 159, 2674, 358, 445, 1631, 2891, 1466, 428, 1553, 2426, 3033, 1822, 702, 283, 1945, 92, 1550, 420]'\n",
      "2 : '[579, 2651, 3301, 1788, 1781, 1327, 1174, 1279, 3429, 1280, 576, 253, 1106, 2664, 1107, 2952, 1108, 1199, 1986, 1120, 1169, 1449, 466, 1483, 632, 699, 1826, 2277, 1934, 982, 538, 2415, 2530, 627, 1059, 2898, 3189, 2785, 1295, 1212, 3379, 1178, 1167, 1007, 1173, 1504, 2162, 3318, 3622]'\n",
      "3 : '[1120, 1025, 3235, 3294, 466, 253, 1106, 1108, 1288, 2488, 1111, 1848, 2739, 2173, 1148, 3186, 1124, 3460, 971]'\n",
      "4 : '[2512, 858, 847, 346, 1158, 2007, 2651, 1050, 2837, 2480, 2785, 3543, 2162, 2701, 2565, 2803, 2554, 1021, 3177, 2897, 2400, 2235, 2748, 3178, 2511, 2140, 2405, 2383, 2126, 2197, 852, 2529, 2518, 907, 1833, 290, 23, 1394, 2364, 2488, 2557, 2202, 2865, 2495, 2959, 2601, 2144, 1889, 2516, 2775, 3550, 3384, 3341, 3281, 3280, 3502, 3556, 487, 1151, 1485, 1848, 980, 2867, 579, 287, 309, 593, 1123, 1017, 49, 1572, 835, 683, 1103, 2374, 804, 1563, 1547, 931, 1430, 754, 2097, 3486, 170, 1621, 33, 1582, 548, 156, 1732, 2090, 1512, 40, 2244, 694, 1087, 1978, 35, 2099, 567, 1342, 1083, 2947, 3043, 447, 31, 3186, 1391, 2832, 1408, 38, 258, 3266, 3036, 1023, 28, 1187, 1743, 1500, 1513, 1384, 1586, 46, 810, 209, 2794, 2130, 2683, 265, 1603, 1413, 1463, 1705, 2166, 1398, 15, 2884, 2708, 1293, 1478, 1593, 483, 1356, 2191, 1584, 1877, 144, 1574, 1581, 699, 2744, 2160, 495, 217, 347, 1325, 338, 312, 1569, 501, 1516, 398, 5, 1506, 3025, 2807, 3243, 1786, 1176, 3155, 1625, 367, 492, 1449, 196, 2078, 1575, 188, 1546, 2864, 1741, 2520, 3042, 1717, 1102, 1585, 2234, 1729, 50, 1406, 343, 537, 222, 1849, 1704, 2889]'\n"
     ]
    }
   ],
   "source": [
    "for i in list(train.keys())[:5]:\n",
    "    print(f\"{i} : '{train[i]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : '[1439]'\n",
      "1 : '[1420]'\n",
      "2 : '[101]'\n",
      "3 : '[2743]'\n",
      "4 : '[1371]'\n"
     ]
    }
   ],
   "source": [
    "for i in list(val.keys())[:5]:\n",
    "    print(f\"{i} : '{val[i]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : '[47]'\n",
      "1 : '[1737]'\n",
      "2 : '[1900]'\n",
      "3 : '[1774]'\n",
      "4 : '[279]'\n"
     ]
    }
   ],
   "source": [
    "for i in list(test.keys())[:5]:\n",
    "    print(f\"{i} : '{test[i]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : '0'\n",
      "2 : '1'\n",
      "3 : '2'\n",
      "4 : '3'\n",
      "5 : '4'\n"
     ]
    }
   ],
   "source": [
    "for i in list(umap.keys())[:5]:\n",
    "    print(f\"{i} : '{umap[i]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : '0'\n",
      "2 : '1'\n",
      "3 : '2'\n",
      "4 : '3'\n",
      "5 : '4'\n"
     ]
    }
   ],
   "source": [
    "for i in list(smap.keys())[:5]:\n",
    "    print(f\"{i} : '{smap[i]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
