{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b7467c9-bde8-4ab3-bc36-f447c9741903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(\"../source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "149adb98-f492-4bd4-9fd4-34e152408dad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf78e3eabf8b4835a4b61b0bf87456e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b2787b8-e411-421e-95b0-a86c73de63f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from data.datasets import dataset_factory\n",
    "from data.dataloader.utils import Prompter\n",
    "from transformers import AutoTokenizer\n",
    "from transformers.models.llama.tokenization_llama import DEFAULT_SYSTEM_PROMPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38596662-b9e8-4a1e-ad4f-ccfee846621a",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "321b84b2-a539-4c9d-bb11-02af1b386c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_init_fn(worker_id):\n",
    "    random.seed(np.random.get_state()[1][0] + worker_id)                                                      \n",
    "    np.random.seed(np.random.get_state()[1][0] + worker_id)\n",
    "\n",
    "\n",
    "# the following prompting is based on alpaca\n",
    "def generate_and_tokenize_eval(args, data_point, tokenizer, prompter):\n",
    "    in_prompt = prompter.generate_prompt(data_point[\"system\"],\n",
    "                                         data_point[\"input\"])\n",
    "    tokenized_full_prompt = tokenizer(in_prompt,\n",
    "                                      truncation=True,\n",
    "                                      max_length=args.llm_max_text_len,\n",
    "                                      padding=False,\n",
    "                                      return_tensors=None)\n",
    "    tokenized_full_prompt[\"labels\"] = ord(data_point[\"output\"]) - ord('A')\n",
    "    \n",
    "    return tokenized_full_prompt\n",
    "\n",
    "\n",
    "def generate_and_tokenize_train(args, data_point, tokenizer, prompter):\n",
    "    def tokenize(prompt, add_eos_token=True):\n",
    "        result = tokenizer(prompt,\n",
    "                           truncation=True,\n",
    "                           max_length=args.llm_max_text_len,\n",
    "                           padding=False,\n",
    "                           return_tensors=None)\n",
    "        if (result[\"input_ids\"][-1] != tokenizer.eos_token_id and add_eos_token):\n",
    "            result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "            result[\"attention_mask\"].append(1)\n",
    "\n",
    "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "        return result\n",
    "\n",
    "    full_prompt = prompter.generate_prompt(data_point[\"system\"],\n",
    "                                           data_point[\"input\"],\n",
    "                                           data_point[\"output\"])\n",
    "    tokenized_full_prompt = tokenize(full_prompt, add_eos_token=True)\n",
    "    if not args.llm_train_on_inputs:\n",
    "        tokenized_full_prompt[\"labels\"][:-2] = [-100] * len(tokenized_full_prompt[\"labels\"][:-2])\n",
    "    \n",
    "    return tokenized_full_prompt\n",
    "\n",
    "\n",
    "def seq_to_token_ids(args, seq, candidates, label, text_dict, tokenizer, prompter, eval=False):\n",
    "    def truncate_title(title):\n",
    "        title_ = tokenizer.tokenize(title)[:args.llm_max_title_len]\n",
    "        title = tokenizer.convert_tokens_to_string(title_)\n",
    "        return title\n",
    "\n",
    "    seq_t = ' \\n '.join(['(' + str(idx + 1) + ') ' + truncate_title(text_dict[item]) \n",
    "                       for idx, item in enumerate(seq)])\n",
    "    can_t = ' \\n '.join(['(' + chr(ord('A') + idx) + ') ' + truncate_title(text_dict[item])\n",
    "                       for idx, item in enumerate(candidates)])\n",
    "    output = chr(ord('A') + candidates.index(label))  # ranking only\n",
    "    \n",
    "    data_point = {}\n",
    "    data_point['system'] = args.llm_system_template if args.llm_system_template is not None else DEFAULT_SYSTEM_PROMPT\n",
    "    data_point['input'] = args.llm_input_template.format(seq_t, can_t)\n",
    "    data_point['output'] = output\n",
    "    \n",
    "    if eval:\n",
    "        return generate_and_tokenize_eval(args, data_point, tokenizer, prompter)\n",
    "    else:\n",
    "        return generate_and_tokenize_train(args, data_point, tokenizer, prompter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff526fea-82b1-43d1-b095-dad262a4d30e",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cec06164-e082-45f4-9efa-cc0cb8eeecc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import easydict\n",
    "\n",
    "# jupyter didn't support argparse. so, I use 'easydict' module\n",
    "args = easydict.EasyDict({\n",
    "    ################\n",
    "    # Dataset\n",
    "    ################\n",
    "    'dataset_code': 'ml-100k', # ml-100k, beauty, games\n",
    "    'min_rating': 0,  # default: 0\n",
    "    'min_uc': 5,  # default: 5\n",
    "    'min_sc': 5,  # default: 5\n",
    "    'seed': 42,  # default: 42\n",
    "\n",
    "    ################\n",
    "    # Dataloader\n",
    "    ################\n",
    "    'train_batch_size': 64,  # default: 64\n",
    "    'val_batch_size': 64,  # default: 64\n",
    "    'test_batch_size': 64,  # default: 64\n",
    "    'num_workers': 0,  # default: 8\n",
    "    'sliding_window_size': 1.0,  # default: 1.0\n",
    "    'negative_sample_size': 10,  # default: 10\n",
    "\n",
    "    ################\n",
    "    # Trainer\n",
    "    ################\n",
    "    # optimization #\n",
    "    'device': 'cuda',  # default: 'cuda'  # choices: ['cpu', 'cuda']\n",
    "    'num_epochs': 500,  # default: 500\n",
    "    'optimizer': 'AdamW',  # default: 'AdamW'  # choices: ['AdamW', 'Adam']\n",
    "    'weight_decay': 0.01,  # default: None\n",
    "    'adam_epsilon': 1e-9,  # default: 1e-9\n",
    "    'momentum': None,  # default: None\n",
    "    'lr': 0.001,  # default: 0.001\n",
    "    'max_grad_norm': 5.0,  # default: 5.0\n",
    "    'enable_lr_schedule': True,  # default: True\n",
    "    'decay_step': 10000,  # default: 10000\n",
    "    'gamma': 1,  # default: 1\n",
    "    'enable_lr_warmup': True,  # default: True\n",
    "    'warmup_steps': 100,  # default: 100\n",
    "\n",
    "    # evaluation #\n",
    "    'val_strategy': 'iteration',  # default: 'iteration'  # choices: ['epoch', 'iteration']\n",
    "    'val_iterations': 500,  # default: 500  # only for iteration val_strategy\n",
    "    'early_stopping': True,  # default: True\n",
    "    'early_stopping_patience': 20,  # default: 20\n",
    "    'metric_ks': [1, 5, 10, 20, 50],  # default: [1, 5, 10, 20, 50]\n",
    "    'rerank_metric_ks': [1, 5, 10],  # default: [1, 5, 10]\n",
    "    'best_metric': 'Recall@10',  # default: 'Recall@10'\n",
    "    'rerank_best_metric': 'NDCG@10',  # default: 'NDCG@10'\n",
    "    'use_wandb': False,  # default: False\n",
    "\n",
    "    ################\n",
    "    # Retriever Model\n",
    "    ################\n",
    "    'model_code': 'llm',  # default: None\n",
    "    'bert_max_len': 50,  # default: 50\n",
    "    'bert_hidden_units': 64,  # default: 64\n",
    "    'bert_num_blocks': 2,  # default: 2\n",
    "    'bert_num_heads': 2,  # default: 2\n",
    "    'bert_head_size': 32,  # default: 32\n",
    "    'bert_dropout': 0.2,  # default: 0.2\n",
    "    'bert_attn_dropout': 0.2,  # default: 0.2\n",
    "    'bert_mask_prob': 0.25,  # default: 0.25\n",
    "\n",
    "    ################\n",
    "    # LLM Model\n",
    "    ################\n",
    "    'llm_base_model': 'meta-llama/Llama-2-7b-hf',  # default: 'meta-llama/Llama-2-7b-hf'\n",
    "    'llm_base_tokenizer': 'meta-llama/Llama-2-7b-hf',  # default: 'meta-llama/Llama-2-7b-hf'\n",
    "    'llm_max_title_len': 32,  # default: 32\n",
    "    'llm_max_text_len': 1536,  # default: 1536\n",
    "    'llm_max_history': 20,  # default: 20\n",
    "    'llm_train_on_inputs': False,  # default: False\n",
    "    'llm_negative_sample_size': 19,  # default: 19  # 19 negative & 1 positive\n",
    "    'llm_system_template': \"Given user history in chronological order, recommend an item from the candidate pool with its index letter.\",  # default: \"Given user history in chronological order, recommend an item from the candidate pool with its index letter.\"\n",
    "    'llm_input_template': 'User history: {}; \\n Candidate pool: {}',  # default: 'User history: {}; \\n Candidate pool: {}'\n",
    "    'llm_load_in_4bit': True,  # default: True\n",
    "    'llm_retrieved_path': \"/home/laststar/data/model/OpenLLM-Rec/retrieved.pkl\",  # default: None\n",
    "    'llm_cache_dir': None,  # default: None\n",
    "\n",
    "    ################\n",
    "    # Lora\n",
    "    ################\n",
    "    'lora_r': 8,  # default: 8\n",
    "    'lora_alpha': 32,  # default: 32\n",
    "    'lora_dropout': 0.05,  # default: 0.05\n",
    "    'lora_target_modules': ['q_proj', 'v_proj'],  # default: ['q_proj', 'v_proj']\n",
    "    'lora_num_epochs': 1,  # default: 1\n",
    "    'lora_val_iterations': 100,  # default: 100\n",
    "    'lora_early_stopping_patience': 20,  # default: 20\n",
    "    'lora_lr': 1e-4,  # default: 1e-4\n",
    "    'lora_micro_batch_size': 16,  # default: 16\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c9a76cb-61cd-4a2e-a905-5012a9914d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already preprocessed. Skip preprocessing\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset_factory(args)\n",
    "dataset = dataset.load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f096ad9e-4b79-4d55-abc8-e7a4c5d14b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_count : 610\n",
      "item_count : 3650\n",
      "max_len : 20\n"
     ]
    }
   ],
   "source": [
    "train = dataset['train']\n",
    "val = dataset['val']\n",
    "test = dataset['test']\n",
    "umap = dataset['umap']\n",
    "smap = dataset['smap']\n",
    "text_dict = dataset['meta']\n",
    "user_count = len(umap)\n",
    "item_count = len(smap)\n",
    "rng = np.random\n",
    "\n",
    "args.num_items = item_count\n",
    "max_len = args.llm_max_history\n",
    "\n",
    "print(f\"user_count : {user_count}\")\n",
    "print(f\"item_count : {item_count}\")\n",
    "print(f\"max_len : {max_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b5f5b53-18b7-4472-8a55-4b1f553d69cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(args.llm_base_tokenizer, cache_dir=args.llm_cache_dir)\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer.padding_side = 'left'\n",
    "tokenizer.truncation_side = 'left'\n",
    "tokenizer.clean_up_tokenization_spaces = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08a99a21-0cc7-4ad0-bffc-f55e6500e676",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompter = Prompter(dir_name = \"../source/data/dataloader/templates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3451dde8-841f-44f4-9ec8-7861196f556b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LRU Output Load\n",
    "llm_retrieved_path = args.llm_retrieved_path\n",
    "retrieved_file = pickle.load(open(args.llm_retrieved_path, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d76d51a-905c-4d36-bddf-91a7c6be03e5",
   "metadata": {},
   "source": [
    "- val_user는 모델이 예측한 점수의 상위 llm_negative_sample_size(20)개 항목에서 정답값이 포함되어 있는 경우의 유저 ID값을 수집\n",
    "- val_candidates는 앞써 추출한 val_user에 해당되는 상위 llm_negative_sample_size(20)개 항목 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54f955da-4423-40e4-8b9f-6f63adf18fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_probs shape : (610, 3651) \n",
      "val_labels shape : (610,) \n",
      "val_users shape : (115,) \n",
      "val_candidates shape : (115, 20) \n"
     ]
    }
   ],
   "source": [
    "# ************* Constructing Validation Subset *************\n",
    "\n",
    "val_probs = retrieved_file['val_probs']\n",
    "val_labels = retrieved_file['val_labels']\n",
    "val_metrics = retrieved_file['val_metrics'] \n",
    "\n",
    "# torch.topk()는 torch.topk는 주어진 텐서에서 상위 k개의 값과 그 인덱스를 반환\n",
    "# p -> probs\n",
    "# l -> labels\n",
    "val_users = [u for u, (p, l) in enumerate(zip(val_probs, val_labels), start=1) \\\n",
    "                          if l in torch.topk(torch.tensor(p), args.llm_negative_sample_size+1).indices]\n",
    "val_candidates = [torch.topk(torch.tensor(val_probs[u-1]), \n",
    "                        args.llm_negative_sample_size+1).indices.tolist() for u in val_users]\n",
    "\n",
    "\n",
    "print(f\"val_probs shape : {np.array(val_probs).shape} \")\n",
    "print(f\"val_labels shape : {np.array(val_labels).shape} \")\n",
    "print(f\"val_users shape : {np.array(val_users).shape} \")\n",
    "print(f\"val_candidates shape : {np.array(val_candidates).shape} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff66104-1912-4f33-ba50-00fe413308c6",
   "metadata": {},
   "source": [
    "- test_user는 모델이 예측한 점수의 상위 llm_negative_sample_size(20)개 항목에서 정답값이 포함되어 있는 경우의 유저 ID값을 수집\n",
    "- test_candidates는 앞써 추출한 val_user에 해당되는 상위 llm_negative_sample_size(20)개 항목 추출\n",
    "- non_test_user = ~test_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40bd8c2e-1cfe-4903-a2b7-db89a2e083e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ************* Constructing Test Subset *************\n",
    "\n",
    "test_probs = retrieved_file['test_probs']\n",
    "test_labels = retrieved_file['test_labels']\n",
    "test_metrics = retrieved_file['test_metrics']\n",
    "\n",
    "# val 수행 작업과 동일\n",
    "test_users = [u for u, (p, l) in enumerate(zip(test_probs, test_labels), start=1) \\\n",
    "                  if l in torch.topk(torch.tensor(p), args.llm_negative_sample_size+1).indices]\n",
    "test_candidates = [torch.topk(torch.tensor(test_probs[u-1]), \n",
    "                        args.llm_negative_sample_size+1).indices.tolist() for u in test_users]\n",
    "# non_test_users = ~test_users\n",
    "non_test_users = [u for u, (p, l) in enumerate(zip(test_probs, test_labels), start=1) \\\n",
    "                               if l not in torch.topk(torch.tensor(p), args.llm_negative_sample_size+1).indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a4c4a0-3930-401e-b2b0-9e0385fdcb70",
   "metadata": {},
   "source": [
    "# Train Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee7be6c9-51e4-4401-bb1f-0667c7638b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = args\n",
    "u2seq = train\n",
    "max_len = max_len\n",
    "num_items = args.num_items\n",
    "rng = rng\n",
    "text_dict = text_dict\n",
    "tokenizer = tokenizer\n",
    "prompter = prompter\n",
    "\n",
    "all_seqs = []\n",
    "for u in sorted(u2seq.keys()):\n",
    "    seq = u2seq[u]\n",
    "    for i in range(2, len(seq)+1):\n",
    "        all_seqs += [seq[:i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7f87856-c8a0-455a-b0aa-de02f80e31b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 39"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d55fbf1-0bf7-4fd1-904c-a18473b80863",
   "metadata": {},
   "source": [
    "- tokens : 특정 유저(u)에 대한 구매 시퀸스\n",
    "- answer: 특정 유저(u)의 다음 구매할<예측할> 아이템 ID (정답 레이블)\n",
    "- original_seq : tokens에서 answer 제외한 시퀸스 (입력 시퀸스)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f868b1e2-c466-4102-be0f-1b706ce5782b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens : [610, 859, 1309, 1682, 1796, 2202, 2219, 2281, 92, 364, 1808, 1882, 195, 777, 297, 185, 1049, 1714, 1546, 2030, 1054, 2040, 1081, 1166, 1667, 2136, 191, 274, 443, 766, 181, 412, 1914, 2312, 1186, 3, 1641, 1621, 788, 820, 902]\n",
      "answer : 902\n",
      "original_seq : [610, 859, 1309, 1682, 1796, 2202, 2219, 2281, 92, 364, 1808, 1882, 195, 777, 297, 185, 1049, 1714, 1546, 2030, 1054, 2040, 1081, 1166, 1667, 2136, 191, 274, 443, 766, 181, 412, 1914, 2312, 1186, 3, 1641, 1621, 788, 820]\n"
     ]
    }
   ],
   "source": [
    "tokens = all_seqs[index]\n",
    "answer = tokens[-1]\n",
    "original_seq = tokens[:-1]\n",
    "\n",
    "print(f\"tokens : {tokens}\")\n",
    "print(f\"answer : {answer}\")\n",
    "print(f\"original_seq : {original_seq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0195d967-3b14-4ff1-8ab5-77ae570a3748",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = original_seq[-max_len:]\n",
    "cur_idx, candidates = 0, [answer]\n",
    "samples = rng.randint(1, args.num_items+1, size=5*args.llm_negative_sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fc1bd8-49f0-45c9-8a4f-dbedcbcf8f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ef9422-2a09-42be-8ef6-25d0178a04e7",
   "metadata": {},
   "source": [
    "# Test Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0dd604-ca5b-4069-ae94-b34b4e83b123",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3ae1b7-1bd1-421a-a60d-f2d030d49058",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b75219f-6241-4b6f-907d-ab8809dea8b6",
   "metadata": {},
   "source": [
    "# Validation Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15ea5b8-17c3-49f7-9082-efc1e44e04a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf34796-49de-42fa-aea4-814342896594",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9a65b0-6aa8-4662-9a1a-de9b35c1fcb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11aa91e5-d98f-4344-834e-6ba8b4d72620",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f71ea19-760b-4116-b57d-43d637b5653d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d853d09-fe94-469f-bf47-e606605d4644",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf33b9a5-cd4d-44f1-bccf-6ea322b307ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a7babe-b0c5-49bb-ae1e-924f0fb12375",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0c29c9-df41-49a6-b51f-d7da007e2b29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b6eb93-167b-4231-a6f7-3484c3be9d1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d96443-555e-4c6c-8a68-2cc773211257",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33e4081-36c6-4a78-a303-026ffbdf725b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "llama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
