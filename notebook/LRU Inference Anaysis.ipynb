{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b66b8942-8a32-435f-a97c-b383beae2392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(\"/home/laststar/source/open-llm-rec/source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1eef45ab-13e7-4e9c-a189-532609420c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import easydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e891b46-d586-44ba-8044-15d41db694e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jupyter didn't support argparse. so, I use 'easydict' module\n",
    "args = easydict.EasyDict({\n",
    "    ################\n",
    "    # Dataset\n",
    "    ################\n",
    "    'dataset_code': 'ml-100k', # ml-100k, beauty, games\n",
    "    'min_rating': 0,  # default: 0\n",
    "    'min_uc': 5,  # default: 5\n",
    "    'min_sc': 5,  # default: 5\n",
    "    'seed': 42,  # default: 42\n",
    "\n",
    "    ################\n",
    "    # Dataloader\n",
    "    ################\n",
    "    'train_batch_size': 64,  # default: 64\n",
    "    'val_batch_size': 64,  # default: 64\n",
    "    'test_batch_size': 64,  # default: 64\n",
    "    'num_workers': 0,  # default: 8\n",
    "    'sliding_window_size': 1.0,  # default: 1.0\n",
    "    'negative_sample_size': 10,  # default: 10\n",
    "\n",
    "    ################\n",
    "    # Trainer\n",
    "    ################\n",
    "    # optimization #\n",
    "    'device': 'cuda',  # default: 'cuda'  # choices: ['cpu', 'cuda']\n",
    "    'num_epochs': 500,  # default: 500\n",
    "    'optimizer': 'AdamW',  # default: 'AdamW'  # choices: ['AdamW', 'Adam']\n",
    "    'weight_decay': 0.01,  # default: None\n",
    "    'adam_epsilon': 1e-9,  # default: 1e-9\n",
    "    'momentum': None,  # default: None\n",
    "    'lr': 0.001,  # default: 0.001\n",
    "    'max_grad_norm': 5.0,  # default: 5.0\n",
    "    'enable_lr_schedule': True,  # default: True\n",
    "    'decay_step': 10000,  # default: 10000\n",
    "    'gamma': 1,  # default: 1\n",
    "    'enable_lr_warmup': True,  # default: True\n",
    "    'warmup_steps': 100,  # default: 100\n",
    "\n",
    "    # evaluation #\n",
    "    'val_strategy': 'iteration',  # default: 'iteration'  # choices: ['epoch', 'iteration']\n",
    "    'val_iterations': 500,  # default: 500  # only for iteration val_strategy\n",
    "    'early_stopping': True,  # default: True\n",
    "    'early_stopping_patience': 20,  # default: 20\n",
    "    'metric_ks': [1, 5, 10, 20, 50],  # default: [1, 5, 10, 20, 50]\n",
    "    'rerank_metric_ks': [1, 5, 10],  # default: [1, 5, 10]\n",
    "    'best_metric': 'Recall@10',  # default: 'Recall@10'\n",
    "    'rerank_best_metric': 'NDCG@10',  # default: 'NDCG@10'\n",
    "    'use_wandb': False,  # default: False\n",
    "\n",
    "    ################\n",
    "    # Retriever Model\n",
    "    ################\n",
    "    'model_code': 'lru',  # default: None\n",
    "    'bert_max_len': 50,  # default: 50\n",
    "    'bert_hidden_units': 64,  # default: 64\n",
    "    'bert_num_blocks': 2,  # default: 2\n",
    "    'bert_num_heads': 2,  # default: 2\n",
    "    'bert_head_size': 32,  # default: 32\n",
    "    'bert_dropout': 0.2,  # default: 0.2\n",
    "    'bert_attn_dropout': 0.2,  # default: 0.2\n",
    "    'bert_mask_prob': 0.25,  # default: 0.25\n",
    "\n",
    "    ################\n",
    "    # LLM Model\n",
    "    ################\n",
    "    'llm_base_model': 'meta-llama/Llama-2-7b-hf',  # default: 'meta-llama/Llama-2-7b-hf'\n",
    "    'llm_base_tokenizer': 'meta-llama/Llama-2-7b-hf',  # default: 'meta-llama/Llama-2-7b-hf'\n",
    "    'llm_max_title_len': 32,  # default: 32\n",
    "    'llm_max_text_len': 1536,  # default: 1536\n",
    "    'llm_max_history': 20,  # default: 20\n",
    "    'llm_train_on_inputs': False,  # default: False\n",
    "    'llm_negative_sample_size': 19,  # default: 19  # 19 negative & 1 positive\n",
    "    'llm_system_template': \"Given user history in chronological order, recommend an item from the candidate pool with its index letter.\",  # default: \"Given user history in chronological order, recommend an item from the candidate pool with its index letter.\"\n",
    "    'llm_input_template': 'User history: {}; \\n Candidate pool: {}',  # default: 'User history: {}; \\n Candidate pool: {}'\n",
    "    'llm_load_in_4bit': True,  # default: True\n",
    "    'llm_retrieved_path': None,  # default: None\n",
    "    'llm_cache_dir': None,  # default: None\n",
    "\n",
    "    ################\n",
    "    # Lora\n",
    "    ################\n",
    "    'lora_r': 8,  # default: 8\n",
    "    'lora_alpha': 32,  # default: 32\n",
    "    'lora_dropout': 0.05,  # default: 0.05\n",
    "    'lora_target_modules': ['q_proj', 'v_proj'],  # default: ['q_proj', 'v_proj']\n",
    "    'lora_num_epochs': 1,  # default: 1\n",
    "    'lora_val_iterations': 100,  # default: 100\n",
    "    'lora_early_stopping_patience': 20,  # default: 20\n",
    "    'lora_lr': 1e-4,  # default: 1e-4\n",
    "    'lora_micro_batch_size': 16,  # default: 16\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda55b3d-1665-4523-b2e7-b5ba3858924f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f441f670-1d6d-4fd8-8bed-22cc1861b9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import data.datasets\n",
    "import data.dataloader\n",
    "from data.dataloader import *\n",
    "from data.datasets import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67a78571-7a6e-48af-a125-775bdfb2ca1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already preprocessed. Skip preprocessing\n"
     ]
    }
   ],
   "source": [
    "train, val, test = dataloader_factory(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "664c6155-e4fd-452a-ab0b-d37b3434f4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PARAMETER_PATH = \"/home/laststar/data/model/open-llm-rec/models/best_acc_model.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ca34dbf-084d-48fe-a034-751e3f14e114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = args.device\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a78c656f-5dd6-4dd6-b506-79a6e02e27a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42b13be5-5bc0-49e6-b60c-ed6f4d634d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "import wandb\n",
    "import argparse\n",
    "\n",
    "from config import *\n",
    "from model import *\n",
    "from data.dataloader import*\n",
    "from trainer import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6090ec2-73ac-4b5f-bb09-be24c2e00b4d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32bdb9db-b699-4ba1-bf3b-05dc5dc49920",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laststar/anaconda3/envs/llama/lib/python3.11/site-packages/torch/nn/modules/module.py:1145: UserWarning: Complex modules are a new feature under active development whose design may change, and some modules might not work as expected when using complex tensors as parameters or buffers. Please file an issue at https://github.com/pytorch/pytorch/issues/new?template=bug-report.yml if a complex module does not work as expected.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = LRURec(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3d86828-c82d-4d2e-a4ad-de88f348bff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a048c519-c8b0-454d-8e76-0a75b202a1c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('embedding.token.weight',\n",
       "              tensor([[-0.0021,  0.0086,  0.0172,  ...,  0.0058,  0.0066,  0.0242],\n",
       "                      [-0.0093,  0.0341, -0.0020,  ..., -0.0095,  0.0187,  0.0108],\n",
       "                      [ 0.0125, -0.0266,  0.0351,  ..., -0.0082,  0.0339,  0.0008],\n",
       "                      ...,\n",
       "                      [-0.0073,  0.0329, -0.0304,  ..., -0.0243,  0.0179,  0.0008],\n",
       "                      [ 0.0162,  0.0032,  0.0313,  ..., -0.0098,  0.0061, -0.0152],\n",
       "                      [ 0.0270,  0.0087, -0.0034,  ...,  0.0110,  0.0135,  0.0345]],\n",
       "                     device='cuda:0')),\n",
       "             ('embedding.layer_norm.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')),\n",
       "             ('embedding.layer_norm.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('model.bias',\n",
       "              tensor([-2.3430e-02, -1.7481e-02, -1.1200e-03,  ..., -6.3832e-06,\n",
       "                       1.0414e-02,  2.8250e-03], device='cuda:0')),\n",
       "             ('model.lru_blocks.0.lru_layer.params_log',\n",
       "              tensor([[-1.8594, -2.3401, -2.7788, -2.3062, -1.7530, -1.9057, -2.7718, -2.5104,\n",
       "                       -1.9486, -2.3579, -2.0880, -1.9073, -1.6406, -2.8914, -4.0121, -1.6374,\n",
       "                       -3.7941, -1.8361, -2.0899, -2.0213, -4.0194, -1.5186, -2.2768, -4.0905,\n",
       "                       -1.8923, -1.7032, -1.9852, -1.6157, -1.6355, -2.4655, -2.3531, -1.9550,\n",
       "                       -2.4588, -2.7089, -1.6455, -2.6615, -2.3614, -3.9095, -2.1558, -2.2581,\n",
       "                       -2.5522, -2.1589, -2.4652, -2.1145, -2.0598, -1.6341, -1.8008, -2.0885,\n",
       "                       -4.2541, -2.2426, -2.1133, -4.4909, -3.1149, -3.9047, -1.7855, -2.9939,\n",
       "                       -2.2815, -2.9571, -1.8266, -3.6813, -2.3096, -2.3108, -2.4439, -2.1544,\n",
       "                       -2.2799, -1.8951, -2.0285, -1.6921, -1.6876, -2.2218, -1.5671, -3.0964,\n",
       "                       -2.1428, -4.2106, -2.3970, -3.4195, -1.6144, -1.7878, -2.6205, -3.2791,\n",
       "                       -2.0715, -3.7608, -2.2511, -3.1402, -2.7987, -2.6383, -3.3241, -1.8706,\n",
       "                       -3.2241, -1.9400, -1.5044, -2.7687, -1.7400, -1.6372, -2.1413, -2.0748,\n",
       "                       -1.5636, -1.8939, -1.8767, -1.8505, -3.0711, -2.7200, -3.9407, -2.8660,\n",
       "                       -1.6933, -3.1104, -3.0541, -1.5912, -1.9826, -1.7538, -2.2704, -1.9278,\n",
       "                       -3.8883, -2.2473, -1.9475, -1.7429, -2.8511, -4.5570, -3.9563, -2.6431,\n",
       "                       -2.9462, -1.6863, -2.6152, -3.0563, -3.1594, -1.6337, -3.3899, -2.9592],\n",
       "                      [ 1.4593,  0.6256, -0.3322,  1.4369,  1.5941,  0.8338,  1.7908, -2.1007,\n",
       "                        0.7212, -1.9884,  1.2654,  0.9109,  1.4347,  1.7803,  1.7242, -3.6122,\n",
       "                        1.1643,  1.7944,  1.2073, -3.0359,  0.6113, -0.3970,  1.2430,  1.8242,\n",
       "                        1.7963,  1.1056,  0.8331,  1.7813,  0.8550,  0.3878,  1.6664,  1.5930,\n",
       "                        1.4224,  1.5215,  1.8061, -0.8971, -0.9678,  0.5143,  1.2561,  0.9883,\n",
       "                        1.7696,  1.5523,  0.7673,  1.7495,  1.8307,  1.0308, -0.2880,  1.7730,\n",
       "                        1.4961,  1.3915,  1.4349,  1.4494,  1.2004,  1.7583,  1.4904,  1.4881,\n",
       "                        1.1135,  1.6741, -1.0795,  0.9540,  1.6039,  1.5531,  1.4739,  1.5791,\n",
       "                        1.7908,  1.6551,  1.3931,  1.2021, -2.2920, -1.0987,  0.7273,  1.4633,\n",
       "                        1.6483,  0.5841,  1.3769, -1.0700,  0.8062,  1.8051, -0.3155,  1.7673,\n",
       "                        1.1934, -1.5326,  1.5345, -1.6730, -0.1710,  0.9691,  1.1775, -1.0668,\n",
       "                        1.7817, -0.3298,  1.1954,  0.6357,  1.6149,  1.4269,  1.1863,  0.2683,\n",
       "                        1.4522, -0.0610,  1.0363,  0.9197,  0.6066,  1.3312,  1.4012,  1.5944,\n",
       "                        1.0946, -3.1000,  0.3814, -1.6318,  1.7408,  1.6130,  1.5561,  1.6021,\n",
       "                       -1.3040,  1.2278,  1.2253,  1.2132,  1.6689,  1.2012,  1.7592,  0.6452,\n",
       "                        0.0533, -4.2618,  0.4510,  1.2704,  1.7222,  1.6905, -1.3777,  0.0915],\n",
       "                      [-0.6590, -0.8709, -1.0735, -0.8555, -0.6141, -0.6788, -1.0703, -0.9487,\n",
       "                       -0.6973, -0.8790, -0.7581, -0.6795, -0.5675, -1.1266, -1.6685, -0.5662,\n",
       "                       -1.5617, -0.6491, -0.7590, -0.7289, -1.6721, -0.5183, -0.8423, -1.7070,\n",
       "                       -0.6730, -0.5933, -0.7131, -0.5574, -0.5654, -0.9281, -0.8768, -0.7000,\n",
       "                       -0.9250, -1.0408, -0.5696, -1.0187, -0.8805, -1.6182, -0.7881, -0.8338,\n",
       "                       -0.9680, -0.7895, -0.9279, -0.7698, -0.7457, -0.5649, -0.6342, -0.7583,\n",
       "                       -1.7876, -0.8269, -0.7693, -1.9045, -1.2329, -1.6158, -0.6277, -1.1752,\n",
       "                       -0.8444, -1.1577, -0.6450, -1.5066, -0.8571, -0.8576, -0.9182, -0.7875,\n",
       "                       -0.8437, -0.6742, -0.7320, -0.5887, -0.5869, -0.8175, -0.5377, -1.2241,\n",
       "                       -0.7823, -1.7661, -0.8967, -1.3795, -0.5568, -0.6287, -0.9996, -1.3117,\n",
       "                       -0.7508, -1.5454, -0.8307, -1.2450, -1.0829, -1.0079, -1.3334, -0.6637,\n",
       "                       -1.2852, -0.6936, -0.5126, -1.0688, -0.6086, -0.5661, -0.7817, -0.7523,\n",
       "                       -0.5363, -0.6737, -0.6664, -0.6552, -1.2120, -1.0460, -1.6335, -1.1146,\n",
       "                       -0.5892, -1.2308, -1.2039, -0.5474, -0.7120, -0.6144, -0.8394, -0.6883,\n",
       "                       -1.6078, -0.8290, -0.6968, -0.6099, -1.1076, -1.9371, -1.6411, -1.0101,\n",
       "                       -1.1526, -0.5863, -0.9972, -1.2049, -1.2542, -0.5647, -1.3651, -1.1587]],\n",
       "                     device='cuda:0')),\n",
       "             ('model.lru_blocks.0.lru_layer.in_proj.weight',\n",
       "              tensor([[-3.5501e-02+0.0177j,  4.3991e-03+0.0218j,  3.1775e-02+0.0164j,\n",
       "                        ...,  2.3918e-02-0.0034j,  2.1836e-05+0.0082j,\n",
       "                       -6.4977e-03+0.0274j],\n",
       "                      [-1.1785e-02+0.0248j, -1.5913e-02-0.0142j, -2.2719e-03-0.0169j,\n",
       "                        ..., -1.2056e-02+0.0003j,  1.7071e-02-0.0194j,\n",
       "                       -2.1950e-02-0.0084j],\n",
       "                      [-3.8509e-03-0.0378j, -1.4885e-02-0.0025j, -1.9281e-02-0.0039j,\n",
       "                        ...,  2.4239e-02+0.0134j,  1.4554e-02-0.0101j,\n",
       "                       -3.6321e-03+0.0136j],\n",
       "                      ...,\n",
       "                      [ 5.5067e-03+0.0050j,  1.0117e-03-0.0190j, -2.0630e-02-0.0148j,\n",
       "                        ...,  2.0975e-02-0.0064j,  1.5136e-02+0.0260j,\n",
       "                        2.9702e-03+0.0080j],\n",
       "                      [ 1.9319e-02+0.0384j,  2.0934e-02+0.0038j, -9.6396e-03+0.0030j,\n",
       "                        ...,  3.3299e-02+0.0388j, -3.1864e-02-0.0147j,\n",
       "                       -1.2147e-02-0.0131j],\n",
       "                      [ 1.1837e-03+0.0267j,  1.3560e-02+0.0362j, -2.1348e-02+0.0106j,\n",
       "                        ..., -2.6928e-04-0.0091j,  1.9738e-02+0.0103j,\n",
       "                       -1.5636e-02-0.0086j]], device='cuda:0')),\n",
       "             ('model.lru_blocks.0.lru_layer.in_proj.bias',\n",
       "              tensor([ 0.0118-1.9274e-02j,  0.0102+1.9278e-02j,  0.0121+9.7192e-03j,\n",
       "                      -0.0189-3.2825e-02j,  0.0232-2.1538e-02j,  0.0050+1.3472e-02j,\n",
       "                      -0.0232+1.9229e-02j,  0.0154+3.1036e-03j, -0.0085-2.8378e-02j,\n",
       "                       0.0144-8.0203e-03j,  0.0135+4.0310e-03j,  0.0161+1.1014e-02j,\n",
       "                      -0.0059-9.2740e-03j,  0.0095+1.2643e-02j, -0.0374+1.9007e-02j,\n",
       "                      -0.0245+8.1894e-03j, -0.0026-1.2954e-02j, -0.0163+1.4463e-04j,\n",
       "                       0.0046+2.5968e-02j, -0.0168-1.7333e-02j,  0.0237-5.5185e-03j,\n",
       "                      -0.0036-1.7292e-02j, -0.0198+8.6123e-03j,  0.0395+3.1334e-02j,\n",
       "                      -0.0016+3.8643e-02j, -0.0214+6.7630e-03j, -0.0167-1.7750e-02j,\n",
       "                       0.0011-2.1912e-02j, -0.0115-7.0708e-03j, -0.0035-9.7988e-03j,\n",
       "                       0.0199-4.1792e-04j,  0.0006-3.2021e-02j,  0.0115-7.7672e-04j,\n",
       "                      -0.0238-1.0032e-02j, -0.0041-1.6611e-02j, -0.0092+2.0964e-02j,\n",
       "                       0.0248-1.9962e-02j,  0.0196+1.7669e-02j,  0.0201+2.4702e-02j,\n",
       "                       0.0102+2.9011e-02j, -0.0227+2.7674e-02j,  0.0213-1.7908e-02j,\n",
       "                       0.0212+3.0082e-02j,  0.0094+8.8207e-03j, -0.0101+3.3510e-02j,\n",
       "                      -0.0029-2.0560e-02j, -0.0023-2.3456e-02j, -0.0300-8.6034e-03j,\n",
       "                       0.0311+5.0608e-03j, -0.0113-5.6210e-03j,  0.0102+2.6007e-02j,\n",
       "                      -0.0310-7.8877e-03j,  0.0269+7.1192e-03j, -0.0028+2.2394e-02j,\n",
       "                      -0.0118+5.9106e-03j,  0.0176+3.6366e-02j, -0.0333-2.8259e-03j,\n",
       "                      -0.0363+3.1394e-02j, -0.0188-3.5133e-02j, -0.0217-1.9990e-02j,\n",
       "                       0.0004+8.6533e-03j, -0.0115+2.4796e-02j,  0.0102+1.3343e-02j,\n",
       "                       0.0058-1.3160e-02j,  0.0052+4.9001e-03j, -0.0038+3.5173e-03j,\n",
       "                       0.0109-8.0733e-03j,  0.0100-1.5774e-02j,  0.0353-8.2704e-03j,\n",
       "                       0.0076+1.8109e-02j,  0.0078+2.1398e-02j,  0.0165+1.9381e-05j,\n",
       "                      -0.0127-3.4118e-04j,  0.0148-8.4743e-03j,  0.0226+1.3173e-02j,\n",
       "                       0.0100+1.2312e-03j,  0.0161+2.6992e-02j, -0.0030-1.2819e-02j,\n",
       "                       0.0206-3.0891e-02j,  0.0252+1.1911e-02j,  0.0140+2.6235e-02j,\n",
       "                       0.0058+3.4074e-02j,  0.0058-7.8931e-03j, -0.0007+1.6224e-02j,\n",
       "                       0.0103-9.5770e-03j, -0.0264-1.9827e-02j,  0.0174+1.9440e-02j,\n",
       "                      -0.0264-2.2736e-02j, -0.0056+2.0277e-02j,  0.0223+3.6905e-03j,\n",
       "                       0.0125+1.6896e-02j, -0.0030+6.7948e-03j, -0.0230-6.3976e-03j,\n",
       "                       0.0236-3.0019e-02j, -0.0189+1.4796e-02j,  0.0020+1.2100e-02j,\n",
       "                      -0.0005-4.7886e-03j, -0.0344+2.2025e-04j,  0.0131-1.2958e-02j,\n",
       "                      -0.0332+2.0353e-02j,  0.0246+2.7809e-02j, -0.0398-1.0612e-02j,\n",
       "                       0.0004+3.4904e-02j, -0.0134-6.2719e-04j, -0.0191-1.5567e-02j,\n",
       "                       0.0186+2.3069e-02j,  0.0010-3.9494e-02j,  0.0044-1.9140e-02j,\n",
       "                       0.0033+1.2017e-02j, -0.0007+2.6838e-02j,  0.0026+2.1872e-02j,\n",
       "                       0.0155+1.0424e-02j, -0.0310+3.2448e-02j,  0.0047-1.3107e-02j,\n",
       "                       0.0060-2.3640e-02j,  0.0058+2.2664e-02j, -0.0077-2.9913e-02j,\n",
       "                       0.0034+3.2897e-02j,  0.0154-1.7181e-02j,  0.0248+4.7436e-03j,\n",
       "                       0.0207-8.2933e-03j,  0.0289-9.4110e-03j,  0.0039+3.2059e-03j,\n",
       "                       0.0095+1.3291e-02j,  0.0064+2.2874e-03j,  0.0158-1.6263e-02j,\n",
       "                       0.0047+5.6464e-04j,  0.0016+4.5051e-03j], device='cuda:0')),\n",
       "             ('model.lru_blocks.0.lru_layer.out_proj.weight',\n",
       "              tensor([[ 0.0371+0.0027j,  0.0360+0.0100j, -0.0111-0.0017j,  ...,\n",
       "                       -0.0068+0.0064j, -0.0129-0.0065j,  0.0285+0.0304j],\n",
       "                      [ 0.0004+0.0180j, -0.0352-0.0024j,  0.0332+0.0027j,  ...,\n",
       "                        0.0031+0.0129j, -0.0076+0.0164j, -0.0352+0.0215j],\n",
       "                      [-0.0001+0.0372j,  0.0107-0.0162j, -0.0165-0.0083j,  ...,\n",
       "                        0.0059+0.0121j, -0.0048-0.0138j,  0.0001-0.0143j],\n",
       "                      ...,\n",
       "                      [-0.0264-0.0041j, -0.0084+0.0079j,  0.0102+0.0236j,  ...,\n",
       "                        0.0246+0.0380j, -0.0237+0.0036j, -0.0150+0.0206j],\n",
       "                      [-0.0214+0.0164j, -0.0007+0.0153j,  0.0365+0.0005j,  ...,\n",
       "                        0.0097-0.0106j, -0.0283+0.0292j, -0.0077-0.0286j],\n",
       "                      [-0.0187+0.0066j,  0.0152-0.0194j, -0.0148-0.0184j,  ...,\n",
       "                        0.0101-0.0015j, -0.0295+0.0020j, -0.0082-0.0079j]], device='cuda:0')),\n",
       "             ('model.lru_blocks.0.lru_layer.out_proj.bias',\n",
       "              tensor([-0.0014+0.0266j,  0.0106+0.0324j,  0.0089+0.0382j, -0.0359+0.0014j,\n",
       "                       0.0071-0.0008j,  0.0204+0.0043j,  0.0147+0.0015j, -0.0226-0.0333j,\n",
       "                       0.0270-0.0015j,  0.0152-0.0168j, -0.0250-0.0172j, -0.0144-0.0234j,\n",
       "                      -0.0183-0.0272j, -0.0150-0.0111j, -0.0180+0.0256j,  0.0034+0.0014j,\n",
       "                      -0.0065+0.0050j, -0.0196-0.0116j,  0.0097+0.0331j, -0.0037+0.0144j,\n",
       "                      -0.0256-0.0142j, -0.0034+0.0330j,  0.0160-0.0047j, -0.0170+0.0306j,\n",
       "                      -0.0147-0.0035j,  0.0150-0.0199j,  0.0138+0.0229j,  0.0136-0.0025j,\n",
       "                      -0.0243-0.0099j,  0.0064-0.0126j,  0.0268+0.0399j, -0.0140-0.0358j,\n",
       "                      -0.0159+0.0159j,  0.0020+0.0041j, -0.0140-0.0367j, -0.0134+0.0079j,\n",
       "                      -0.0072+0.0183j,  0.0210+0.0219j, -0.0006+0.0109j,  0.0226-0.0246j,\n",
       "                      -0.0127-0.0167j,  0.0206+0.0242j, -0.0077-0.0046j,  0.0115+0.0047j,\n",
       "                      -0.0160+0.0177j, -0.0026+0.0360j,  0.0053-0.0233j,  0.0369+0.0151j,\n",
       "                       0.0040-0.0229j, -0.0013+0.0221j, -0.0090-0.0352j,  0.0241+0.0330j,\n",
       "                       0.0024-0.0036j,  0.0112-0.0149j,  0.0171+0.0361j,  0.0004-0.0181j,\n",
       "                       0.0019-0.0049j,  0.0209-0.0001j, -0.0253+0.0253j, -0.0157-0.0211j,\n",
       "                       0.0164-0.0011j,  0.0099-0.0002j,  0.0117+0.0122j,  0.0049+0.0322j],\n",
       "                     device='cuda:0')),\n",
       "             ('model.lru_blocks.0.lru_layer.layer_norm.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')),\n",
       "             ('model.lru_blocks.0.lru_layer.layer_norm.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('model.lru_blocks.0.feed_forward.w_1.weight',\n",
       "              tensor([[-0.0075,  0.0028,  0.0112,  ..., -0.0214, -0.0117,  0.0092],\n",
       "                      [ 0.0048,  0.0170, -0.0042,  ..., -0.0270, -0.0305, -0.0036],\n",
       "                      [-0.0033, -0.0390, -0.0080,  ..., -0.0279,  0.0207, -0.0090],\n",
       "                      ...,\n",
       "                      [-0.0020, -0.0198, -0.0138,  ..., -0.0077, -0.0198, -0.0282],\n",
       "                      [-0.0115,  0.0079,  0.0393,  ..., -0.0191,  0.0232,  0.0071],\n",
       "                      [-0.0270, -0.0273, -0.0087,  ...,  0.0081, -0.0090,  0.0256]],\n",
       "                     device='cuda:0')),\n",
       "             ('model.lru_blocks.0.feed_forward.w_1.bias',\n",
       "              tensor([-2.9541e-02,  1.8221e-02, -4.4315e-03, -6.2868e-03,  1.3186e-02,\n",
       "                       2.4461e-02,  3.4845e-02,  1.8640e-02, -3.0614e-02, -8.3396e-03,\n",
       "                       1.1455e-02,  1.2210e-02,  4.0691e-03,  2.4274e-02, -2.8837e-02,\n",
       "                      -2.4098e-02,  1.1875e-02, -3.3928e-02,  1.7025e-02,  1.7991e-02,\n",
       "                      -1.5266e-02, -1.6933e-02, -1.6865e-02,  2.9316e-03,  7.7186e-03,\n",
       "                      -2.4353e-03, -2.3531e-03, -3.1887e-02, -1.3546e-02, -1.3394e-02,\n",
       "                       6.5042e-03,  2.4527e-02,  1.0702e-02, -2.6217e-03,  2.9659e-02,\n",
       "                       8.4140e-03, -2.6975e-02, -3.3442e-02,  5.2216e-03, -2.5001e-03,\n",
       "                       2.3237e-02,  1.2470e-02, -1.1011e-02,  2.7478e-02,  2.1337e-02,\n",
       "                       1.0840e-02,  2.4042e-02,  2.2934e-02, -1.8026e-02,  1.3119e-03,\n",
       "                       1.3428e-03, -1.2758e-02, -2.3496e-02,  5.2925e-03,  4.4896e-03,\n",
       "                      -3.5593e-02, -1.0137e-02,  2.0988e-02,  2.8209e-02,  3.4644e-02,\n",
       "                       3.4376e-02, -1.8746e-03, -2.6789e-02, -2.0036e-02, -3.1640e-02,\n",
       "                      -1.1231e-04, -1.3620e-02, -1.2735e-03, -5.0953e-03, -3.0669e-02,\n",
       "                      -4.2270e-03,  1.4778e-02, -1.2091e-04, -2.6978e-02,  4.4586e-03,\n",
       "                       8.6425e-03, -2.4901e-02, -2.0005e-02, -1.4979e-02,  1.3428e-02,\n",
       "                      -1.2391e-02,  1.0412e-02,  9.5432e-03, -1.5649e-02, -3.2343e-02,\n",
       "                      -8.5012e-03,  5.1738e-05,  1.8731e-02, -2.7773e-02,  2.6366e-02,\n",
       "                      -1.8688e-03, -2.5577e-02,  5.6144e-03,  1.2269e-02,  4.9900e-03,\n",
       "                      -1.0417e-02,  1.4774e-02, -4.0435e-03,  3.1335e-02,  5.2865e-03,\n",
       "                      -1.3249e-02,  1.8869e-02, -3.3507e-03,  1.1198e-02,  9.0918e-03,\n",
       "                       1.0892e-02,  1.3655e-02, -9.4659e-03,  1.3062e-02, -7.9455e-03,\n",
       "                      -1.9823e-02, -3.1433e-02,  2.1393e-02, -7.5128e-03, -1.4658e-02,\n",
       "                       2.4663e-02,  1.9075e-02, -3.1794e-03,  1.4619e-02, -3.6883e-02,\n",
       "                      -1.1273e-02, -5.5805e-03, -7.8142e-03, -7.6396e-03,  1.4548e-02,\n",
       "                      -1.9070e-02, -3.5763e-02, -1.9653e-02,  9.0906e-03,  3.0827e-02,\n",
       "                      -1.7547e-02, -1.7957e-02, -8.9851e-03,  7.8770e-03,  4.4094e-03,\n",
       "                       1.8343e-04, -6.5244e-03,  3.0970e-03,  1.7933e-03, -8.1383e-04,\n",
       "                      -2.5730e-02, -1.0168e-02, -2.0952e-03, -2.1156e-02,  1.1344e-02,\n",
       "                      -4.9416e-03, -3.5060e-02,  2.3594e-02, -9.4192e-03,  2.4474e-02,\n",
       "                      -3.5458e-02, -9.8195e-03,  3.3298e-02, -3.4326e-02, -8.5172e-03,\n",
       "                      -4.7122e-03,  2.8168e-03,  7.5456e-03, -9.5383e-03, -1.1173e-02,\n",
       "                       2.7899e-02, -1.4382e-03,  6.6713e-03, -2.2054e-03, -1.1310e-02,\n",
       "                      -4.5674e-03,  3.4317e-02,  2.1174e-02, -4.1710e-03, -2.8061e-02,\n",
       "                      -1.0109e-02,  3.5761e-02, -2.1259e-02,  1.0826e-02,  1.1405e-02,\n",
       "                      -1.2307e-02, -1.5093e-02,  1.8057e-02, -3.7195e-04, -2.1043e-02,\n",
       "                      -1.5455e-02,  1.4096e-02,  1.9825e-02, -2.3450e-02,  1.7812e-02,\n",
       "                       2.1119e-02, -1.8018e-02,  1.0225e-02,  2.0583e-02, -2.1911e-02,\n",
       "                      -3.1374e-02,  6.7769e-03,  2.3726e-02, -4.7784e-03, -1.1143e-02,\n",
       "                       3.0102e-03, -1.9202e-02, -3.9656e-03, -6.3561e-03,  2.2700e-02,\n",
       "                       2.9131e-02,  1.1095e-03, -2.5540e-02, -3.0236e-02, -3.4670e-02,\n",
       "                      -1.9446e-02,  2.5345e-02, -3.4405e-02, -5.2688e-03,  2.9688e-02,\n",
       "                       5.0354e-03, -1.4639e-02, -3.5254e-02, -2.3964e-02, -1.5109e-02,\n",
       "                       2.4479e-02, -7.4888e-03, -3.2958e-02, -3.2992e-02, -1.1434e-02,\n",
       "                       6.0199e-03,  1.0472e-02, -4.0930e-04, -2.5890e-02,  2.7915e-02,\n",
       "                       1.4838e-02, -2.7828e-02,  2.7885e-02, -3.9306e-03,  1.6377e-04,\n",
       "                      -2.3037e-02, -2.0246e-02, -6.4446e-04, -1.1909e-02, -1.2631e-02,\n",
       "                       1.9811e-02,  1.6269e-02, -8.2959e-03,  3.5247e-02, -1.8002e-02,\n",
       "                      -5.1521e-03,  3.6561e-02,  2.4172e-02, -1.6451e-03,  2.5770e-02,\n",
       "                      -2.0914e-03,  2.3965e-02,  1.8051e-03, -3.1506e-02, -5.7163e-03,\n",
       "                       9.6816e-03, -2.4849e-03,  3.2131e-02,  8.4690e-04, -2.5945e-02,\n",
       "                      -6.2598e-03], device='cuda:0')),\n",
       "             ('model.lru_blocks.0.feed_forward.w_2.weight',\n",
       "              tensor([[ 0.0081, -0.0008, -0.0216,  ...,  0.0086, -0.0016,  0.0190],\n",
       "                      [-0.0116, -0.0076,  0.0113,  ...,  0.0035, -0.0120,  0.0028],\n",
       "                      [ 0.0123,  0.0136,  0.0072,  ..., -0.0111,  0.0034,  0.0234],\n",
       "                      ...,\n",
       "                      [-0.0103, -0.0139,  0.0100,  ..., -0.0048, -0.0174,  0.0261],\n",
       "                      [-0.0344, -0.0091, -0.0110,  ..., -0.0084, -0.0017,  0.0081],\n",
       "                      [-0.0042, -0.0334, -0.0393,  ..., -0.0133,  0.0009,  0.0147]],\n",
       "                     device='cuda:0')),\n",
       "             ('model.lru_blocks.0.feed_forward.w_2.bias',\n",
       "              tensor([ 0.0148,  0.0078,  0.0026,  0.0151,  0.0116, -0.0168,  0.0069, -0.0107,\n",
       "                       0.0122, -0.0084, -0.0223, -0.0249, -0.0047, -0.0292, -0.0178,  0.0326,\n",
       "                      -0.0079,  0.0257, -0.0231,  0.0087,  0.0297,  0.0070,  0.0148, -0.0022,\n",
       "                       0.0130,  0.0064,  0.0115,  0.0287,  0.0171,  0.0211, -0.0072, -0.0057,\n",
       "                      -0.0134,  0.0399,  0.0209,  0.0203, -0.0136,  0.0209, -0.0182,  0.0242,\n",
       "                       0.0019, -0.0212, -0.0199,  0.0283,  0.0291,  0.0016, -0.0097,  0.0240,\n",
       "                      -0.0089, -0.0155,  0.0089,  0.0038,  0.0087,  0.0176, -0.0244, -0.0394,\n",
       "                       0.0065,  0.0270,  0.0211, -0.0102, -0.0222, -0.0139,  0.0364, -0.0217],\n",
       "                     device='cuda:0')),\n",
       "             ('model.lru_blocks.0.feed_forward.layer_norm.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')),\n",
       "             ('model.lru_blocks.0.feed_forward.layer_norm.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('model.lru_blocks.1.lru_layer.params_log',\n",
       "              tensor([[-3.5527, -2.3683, -2.4313, -1.7601, -2.0439, -1.5514, -2.4950, -1.5815,\n",
       "                       -3.6407, -3.0170, -3.6117, -2.3402, -2.4540, -1.6318, -3.5343, -2.2063,\n",
       "                       -1.9434, -3.4715, -2.7945, -1.5609, -1.7200, -4.3761, -1.8685, -1.6006,\n",
       "                       -1.6625, -4.1353, -3.4493, -2.2283, -1.8092, -2.0281, -1.5477, -3.7378,\n",
       "                       -4.2882, -2.0071, -1.6327, -2.8457, -3.1720, -2.2094, -3.7773, -3.5791,\n",
       "                       -3.1275, -3.6492, -2.6069, -2.7334, -3.9139, -2.6844, -1.5085, -3.4541,\n",
       "                       -3.1029, -2.2965, -1.6776, -1.5149, -2.6019, -2.4160, -1.8516, -1.9173,\n",
       "                       -3.2710, -2.5169, -1.5996, -2.3356, -3.2204, -2.2288, -1.7511, -1.7922,\n",
       "                       -3.1200, -1.7153, -4.5904, -3.0461, -2.8226, -4.4775, -2.2801, -2.3254,\n",
       "                       -1.7827, -1.9887, -2.4208, -2.5536, -2.9721, -3.5461, -2.0515, -2.0650,\n",
       "                       -1.6931, -3.6668, -1.8688, -2.7477, -1.8483, -2.8651, -1.8433, -2.7273,\n",
       "                       -2.0253, -3.3614, -1.6683, -2.9873, -3.9703, -2.4807, -3.4212, -2.5683,\n",
       "                       -2.2622, -3.3307, -2.0972, -1.8804, -1.5738, -1.9541, -3.9807, -1.6200,\n",
       "                       -2.1639, -2.7805, -2.2426, -2.2173, -1.6549, -3.7430, -1.6551, -1.7076,\n",
       "                       -2.3717, -1.9823, -2.4336, -1.5565, -2.0274, -1.7559, -1.8671, -1.8661,\n",
       "                       -2.0992, -2.5015, -2.6066, -2.5609, -2.1147, -1.7671, -3.1907, -1.7144],\n",
       "                      [ 1.1915,  1.7654,  1.7057,  1.7436,  0.4698,  1.6281,  1.4756,  1.2724,\n",
       "                       -0.0392,  1.5755,  1.3176,  1.5129, -4.2218,  1.0489, -2.9616,  1.6007,\n",
       "                       -0.3128,  0.5504,  0.7588,  0.0839, -3.0054,  0.4682, -0.8397, -0.3915,\n",
       "                        0.9130,  1.7653,  1.7758, -0.1004,  1.6176, -1.5170,  0.4851,  1.1750,\n",
       "                       -3.6745,  1.5376,  0.4868,  0.5710,  1.8013,  0.8710,  1.6483,  0.6691,\n",
       "                        0.5909,  1.3722,  1.8169,  0.5354,  1.4433,  1.1647,  1.1600, -1.4904,\n",
       "                       -1.3484, -0.0070,  0.7726,  0.2574,  1.3696,  1.4004,  1.1836,  0.9690,\n",
       "                       -1.9242,  1.7186,  1.7056,  1.6623,  1.6652,  1.1851,  0.4313,  0.7513,\n",
       "                        0.9109,  1.4377,  1.7911,  1.2362,  0.2964,  1.1390,  1.7917,  0.3366,\n",
       "                        1.7918,  1.3214,  1.2922, -2.4501, -0.2157,  1.3802, -0.5463,  1.4785,\n",
       "                        1.5932, -0.1322, -0.7294,  1.3059,  0.5495, -2.8746,  1.2330, -0.7673,\n",
       "                        0.4647,  1.7946,  1.4346,  1.7030,  0.6933,  0.6173,  0.4349,  1.6879,\n",
       "                        1.1771,  0.5961,  1.5522,  1.1890,  1.3872,  0.8866,  1.7798,  1.0271,\n",
       "                        0.7337,  1.7228,  1.0665,  0.3859,  0.6197,  1.3872,  0.4729,  0.9737,\n",
       "                        0.7790,  1.7980,  0.7406, -0.6471,  1.2961,  1.7347, -1.4036, -0.4636,\n",
       "                        0.2983,  0.5058,  1.3734,  0.9180,  1.5281,  1.7242,  1.1730,  1.2166],\n",
       "                      [-1.4440, -0.8837, -0.9124, -0.6170, -0.7388, -0.5314, -0.9416, -0.5435,\n",
       "                       -1.4868, -1.1862, -1.4727, -0.8709, -0.9228, -0.5639, -1.4351, -0.8106,\n",
       "                       -0.6950, -1.4046, -1.0809, -0.5352, -0.6003, -1.8477, -0.6629, -0.5512,\n",
       "                       -0.5765, -1.7291, -1.3939, -0.8205, -0.6377, -0.7318, -0.5299, -1.5342,\n",
       "                       -1.8044, -0.7227, -0.5643, -1.1050, -1.2602, -0.8120, -1.5535, -1.4569,\n",
       "                       -1.2389, -1.4910, -0.9933, -1.0523, -1.6203, -1.0294, -0.5142, -1.3962,\n",
       "                       -1.2271, -0.8512, -0.5827, -0.5168, -0.9910, -0.9054, -0.6557, -0.6838,\n",
       "                       -1.3078, -0.9517, -0.5508, -0.8688, -1.2834, -0.8207, -0.6132, -0.6305,\n",
       "                       -1.2353, -0.5983, -1.9537, -1.2000, -1.0941, -1.8978, -0.8437, -0.8642,\n",
       "                       -0.6265, -0.7147, -0.9076, -0.9686, -1.1649, -1.4408, -0.7421, -0.7480,\n",
       "                       -0.5891, -1.4995, -0.6630, -1.0590, -0.6543, -1.1142, -0.6521, -1.0494,\n",
       "                       -0.7306, -1.3513, -0.5789, -1.1721, -1.6480, -0.9350, -1.3803, -0.9754,\n",
       "                       -0.8357, -1.3366, -0.7622, -0.6680, -0.5404, -0.6996, -1.6531, -0.5591,\n",
       "                       -0.7917, -1.0744, -0.8269, -0.8156, -0.5734, -1.5367, -0.5735, -0.5951,\n",
       "                       -0.8852, -0.7119, -0.9135, -0.5334, -0.7315, -0.6153, -0.6623, -0.6618,\n",
       "                       -0.7631, -0.9446, -0.9932, -0.9720, -0.7699, -0.6200, -1.2692, -0.5980]],\n",
       "                     device='cuda:0')),\n",
       "             ('model.lru_blocks.1.lru_layer.in_proj.weight',\n",
       "              tensor([[ 2.9378e-02+0.0095j,  1.5614e-02+0.0075j,  1.4958e-02-0.0006j,\n",
       "                        ...,  2.9346e-02+0.0312j,  1.6278e-02+0.0073j,\n",
       "                        2.2547e-02+0.0119j],\n",
       "                      [ 2.7707e-03-0.0130j,  1.3366e-02+0.0049j,  3.9168e-03+0.0276j,\n",
       "                        ..., -6.9210e-03-0.0193j,  5.8829e-03-0.0051j,\n",
       "                       -1.8309e-02-0.0142j],\n",
       "                      [-1.9217e-02+0.0148j, -1.1036e-03-0.0068j,  2.4680e-02-0.0022j,\n",
       "                        ..., -6.2685e-04+0.0027j,  2.2177e-02-0.0256j,\n",
       "                       -2.4594e-03-0.0095j],\n",
       "                      ...,\n",
       "                      [-6.1311e-03+0.0069j,  1.1227e-02+0.0108j,  3.3768e-03+0.0223j,\n",
       "                        ...,  1.8883e-02+0.0243j,  3.3822e-02-0.0007j,\n",
       "                       -7.5368e-03-0.0365j],\n",
       "                      [ 8.5519e-03-0.0019j, -1.8799e-02-0.0243j, -1.6609e-02+0.0189j,\n",
       "                        ...,  1.7666e-03+0.0151j,  2.2732e-03-0.0156j,\n",
       "                       -1.2521e-03+0.0186j],\n",
       "                      [ 1.3125e-04-0.0128j,  2.1834e-02-0.0059j, -9.9140e-04+0.0218j,\n",
       "                        ..., -3.1415e-02-0.0001j, -1.6969e-02-0.0064j,\n",
       "                       -4.3364e-05+0.0121j]], device='cuda:0')),\n",
       "             ('model.lru_blocks.1.lru_layer.in_proj.bias',\n",
       "              tensor([-0.0050+2.4936e-03j, -0.0048+2.4852e-04j, -0.0109-2.7228e-03j,\n",
       "                      -0.0179+1.5739e-03j,  0.0135+1.6757e-02j, -0.0138+1.2134e-02j,\n",
       "                      -0.0309-2.1873e-02j,  0.0330-3.5050e-03j, -0.0236-1.8804e-02j,\n",
       "                      -0.0065+5.2126e-03j,  0.0081+2.7541e-02j,  0.0135-1.1502e-02j,\n",
       "                      -0.0165-3.9847e-02j, -0.0074-1.8782e-02j,  0.0069+4.0933e-03j,\n",
       "                      -0.0261+1.1727e-02j,  0.0140+9.3541e-03j,  0.0301-2.7544e-02j,\n",
       "                      -0.0188-2.4009e-02j, -0.0041-5.5508e-03j, -0.0003-2.1487e-02j,\n",
       "                      -0.0244+2.2784e-02j,  0.0288-6.6060e-03j,  0.0167-1.6500e-02j,\n",
       "                       0.0124+6.0566e-03j,  0.0161+1.0213e-02j, -0.0066-2.1071e-02j,\n",
       "                       0.0033+1.1614e-02j,  0.0311+5.2023e-03j, -0.0279+1.2321e-02j,\n",
       "                       0.0143-9.9245e-05j, -0.0133-7.7620e-03j, -0.0105+3.0069e-02j,\n",
       "                       0.0033+3.9947e-03j, -0.0159+2.9436e-02j, -0.0184+6.6887e-04j,\n",
       "                      -0.0240-1.4419e-02j,  0.0185-2.3717e-02j,  0.0143+3.5760e-04j,\n",
       "                      -0.0027-3.8920e-02j,  0.0144+2.7727e-02j,  0.0292-1.3439e-02j,\n",
       "                       0.0207+2.3338e-02j, -0.0184+1.7817e-02j,  0.0070+1.9446e-02j,\n",
       "                       0.0040-1.6626e-02j,  0.0267-9.2044e-03j,  0.0172-2.0036e-02j,\n",
       "                       0.0182-2.5669e-02j, -0.0159-2.7636e-02j, -0.0087+3.7028e-02j,\n",
       "                      -0.0214+1.5119e-02j, -0.0032-5.3790e-03j, -0.0227-8.5655e-03j,\n",
       "                       0.0266+1.4599e-02j,  0.0371-2.8226e-03j, -0.0023+6.2665e-03j,\n",
       "                      -0.0040+2.3612e-02j, -0.0065+3.3934e-02j, -0.0051-1.6974e-02j,\n",
       "                       0.0176+6.5648e-03j,  0.0040-1.7745e-02j,  0.0155-2.0146e-02j,\n",
       "                      -0.0323+1.4672e-02j,  0.0154+2.1358e-02j, -0.0173+1.4942e-03j,\n",
       "                      -0.0023+3.5734e-02j,  0.0262+1.0564e-02j, -0.0214-1.6472e-02j,\n",
       "                      -0.0087+1.5082e-02j,  0.0029+1.3909e-02j,  0.0095+2.4948e-02j,\n",
       "                       0.0140-6.3442e-03j, -0.0190+1.5565e-02j, -0.0164+2.0770e-02j,\n",
       "                       0.0101+2.9221e-02j, -0.0017+2.4462e-02j,  0.0264-1.1195e-02j,\n",
       "                       0.0196+1.2336e-02j, -0.0197-1.4574e-02j, -0.0211-3.9963e-02j,\n",
       "                       0.0065+2.2499e-02j,  0.0356-1.9452e-02j, -0.0097+2.9870e-03j,\n",
       "                      -0.0212+1.4722e-02j,  0.0130-4.0641e-03j, -0.0190+3.4980e-02j,\n",
       "                      -0.0112-1.0211e-02j,  0.0122+6.0377e-03j,  0.0076+1.0209e-02j,\n",
       "                      -0.0376-4.7292e-05j, -0.0055+1.3539e-02j,  0.0172+2.1631e-02j,\n",
       "                      -0.0246+5.3589e-03j,  0.0087-2.4976e-02j, -0.0120+1.2818e-02j,\n",
       "                      -0.0194-1.6795e-02j, -0.0073+9.9355e-03j,  0.0021-7.3030e-03j,\n",
       "                       0.0044-1.7414e-02j, -0.0085-2.0032e-03j, -0.0081+4.1985e-03j,\n",
       "                       0.0392-7.5691e-03j,  0.0288-1.8763e-03j,  0.0134-1.4813e-02j,\n",
       "                      -0.0162+1.7269e-02j,  0.0111+8.9402e-03j, -0.0177-1.6970e-02j,\n",
       "                       0.0103-2.4148e-03j, -0.0261+3.2963e-03j, -0.0148-8.5503e-03j,\n",
       "                       0.0317+4.0698e-04j, -0.0145-4.4059e-03j,  0.0009+2.0063e-02j,\n",
       "                      -0.0034+8.1994e-03j,  0.0078-1.7640e-02j,  0.0268+9.0155e-03j,\n",
       "                       0.0209-9.2143e-04j,  0.0028+4.7942e-03j, -0.0076-1.7373e-02j,\n",
       "                       0.0085+1.4257e-02j,  0.0080-2.4140e-02j,  0.0293-6.5248e-03j,\n",
       "                       0.0263-5.2103e-04j,  0.0027+8.9025e-04j,  0.0209+3.1513e-03j,\n",
       "                      -0.0199-1.1520e-02j, -0.0265+2.3992e-03j], device='cuda:0')),\n",
       "             ('model.lru_blocks.1.lru_layer.out_proj.weight',\n",
       "              tensor([[-0.0264+0.0180j, -0.0149-0.0201j, -0.0004-0.0334j,  ...,\n",
       "                        0.0049+0.0125j,  0.0002-0.0241j, -0.0010+0.0034j],\n",
       "                      [ 0.0231-0.0068j,  0.0071+0.0328j,  0.0136+0.0242j,  ...,\n",
       "                        0.0002+0.0289j, -0.0076-0.0034j,  0.0173-0.0251j],\n",
       "                      [ 0.0254+0.0350j, -0.0296+0.0251j,  0.0146+0.0143j,  ...,\n",
       "                        0.0201-0.0137j, -0.0165+0.0211j, -0.0169-0.0009j],\n",
       "                      ...,\n",
       "                      [-0.0084-0.0148j, -0.0057-0.0038j,  0.0325-0.0016j,  ...,\n",
       "                        0.0194+0.0146j,  0.0118+0.0009j,  0.0061-0.0197j],\n",
       "                      [ 0.0129-0.0142j,  0.0112+0.0050j, -0.0170+0.0199j,  ...,\n",
       "                        0.0165-0.0055j,  0.0173-0.0394j, -0.0014+0.0051j],\n",
       "                      [ 0.0099-0.0066j, -0.0213-0.0151j, -0.0283+0.0097j,  ...,\n",
       "                        0.0025+0.0078j,  0.0117-0.0077j, -0.0297+0.0153j]], device='cuda:0')),\n",
       "             ('model.lru_blocks.1.lru_layer.out_proj.bias',\n",
       "              tensor([-1.8737e-02-0.0030j, -2.5853e-02+0.0142j, -2.0274e-02+0.0206j,\n",
       "                      -1.3054e-03+0.0207j,  1.1599e-02+0.0128j,  1.9258e-02+0.0176j,\n",
       "                      -3.0335e-02-0.0006j, -3.2059e-02+0.0155j, -4.5996e-03-0.0013j,\n",
       "                      -3.1825e-05-0.0053j,  8.5685e-04+0.0153j,  3.4504e-02+0.0025j,\n",
       "                      -4.7666e-03-0.0056j, -3.1961e-02-0.0061j, -3.1400e-02-0.0015j,\n",
       "                      -6.6143e-03+0.0075j,  1.0314e-02-0.0341j, -9.6634e-03-0.0014j,\n",
       "                       3.2478e-02-0.0151j, -2.2974e-02-0.0164j,  2.6692e-02+0.0160j,\n",
       "                      -1.9964e-03-0.0063j,  3.7485e-03-0.0247j, -4.8563e-03+0.0237j,\n",
       "                       1.0229e-02+0.0011j, -8.6242e-03-0.0066j,  3.5287e-02-0.0133j,\n",
       "                      -3.2213e-02+0.0093j, -2.4998e-02+0.0266j, -4.0106e-03-0.0172j,\n",
       "                      -6.4712e-04-0.0093j, -1.7018e-02-0.0012j, -3.7724e-03+0.0092j,\n",
       "                      -2.0077e-02+0.0028j, -2.1622e-02-0.0020j,  2.3116e-02-0.0322j,\n",
       "                       1.7873e-02-0.0353j,  2.5632e-03+0.0209j,  1.8974e-02+0.0081j,\n",
       "                       3.0835e-02+0.0089j,  2.1103e-02+0.0130j,  5.8066e-03+0.0019j,\n",
       "                       2.5327e-02+0.0304j, -2.3406e-02-0.0008j, -1.0305e-03-0.0279j,\n",
       "                      -7.4707e-03-0.0012j, -2.5252e-02-0.0240j, -8.9248e-03-0.0082j,\n",
       "                      -1.4642e-02+0.0078j, -2.5790e-02-0.0318j, -1.1047e-02-0.0355j,\n",
       "                       8.2612e-03-0.0032j,  2.0109e-03-0.0388j,  1.9766e-02+0.0112j,\n",
       "                       1.9109e-02+0.0273j, -1.0130e-02-0.0152j,  1.2581e-02+0.0166j,\n",
       "                      -1.1778e-02-0.0072j, -4.8942e-03-0.0087j,  3.5070e-03+0.0182j,\n",
       "                       9.2257e-03+0.0062j, -1.1813e-02+0.0095j,  2.4206e-03-0.0225j,\n",
       "                       9.2373e-03-0.0181j], device='cuda:0')),\n",
       "             ('model.lru_blocks.1.lru_layer.layer_norm.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')),\n",
       "             ('model.lru_blocks.1.lru_layer.layer_norm.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0')),\n",
       "             ('model.lru_blocks.1.feed_forward.w_1.weight',\n",
       "              tensor([[-0.0160, -0.0130,  0.0126,  ..., -0.0011,  0.0122, -0.0173],\n",
       "                      [-0.0316,  0.0078, -0.0288,  ..., -0.0246,  0.0382, -0.0188],\n",
       "                      [-0.0292, -0.0054,  0.0183,  ..., -0.0067, -0.0210, -0.0212],\n",
       "                      ...,\n",
       "                      [ 0.0308, -0.0039, -0.0282,  ...,  0.0031, -0.0034,  0.0009],\n",
       "                      [-0.0297,  0.0009,  0.0008,  ...,  0.0168, -0.0198,  0.0058],\n",
       "                      [-0.0144,  0.0122,  0.0026,  ...,  0.0039, -0.0102,  0.0133]],\n",
       "                     device='cuda:0')),\n",
       "             ('model.lru_blocks.1.feed_forward.w_1.bias',\n",
       "              tensor([ 5.8600e-03, -1.3997e-03, -3.5583e-04, -1.5211e-02, -2.8828e-02,\n",
       "                       1.7293e-02, -7.5403e-03,  6.0909e-03, -5.2621e-03, -4.4635e-04,\n",
       "                      -1.3428e-02,  1.8452e-02,  4.6962e-03,  1.2286e-02, -1.8342e-02,\n",
       "                      -3.4058e-03,  3.8591e-02,  1.3782e-02,  1.2111e-02,  7.2656e-04,\n",
       "                       3.9631e-03,  3.6627e-02,  1.5451e-03, -7.5821e-03,  3.1156e-02,\n",
       "                       2.3395e-03,  5.7173e-03,  4.1288e-03,  3.4752e-02,  1.6584e-02,\n",
       "                      -2.4640e-04, -7.7741e-03, -9.4411e-03, -2.8909e-02, -1.9975e-02,\n",
       "                       1.7157e-02,  2.8839e-02,  9.1143e-03,  1.2504e-02, -1.9915e-02,\n",
       "                       3.9290e-02,  2.0805e-04,  3.6038e-02, -2.5760e-02, -7.3660e-03,\n",
       "                       3.6919e-02,  1.5951e-02,  6.0713e-03, -9.6381e-04,  1.0032e-02,\n",
       "                      -7.0419e-03,  3.8400e-02, -3.2189e-02,  1.3410e-02, -1.9896e-02,\n",
       "                      -2.0383e-02, -2.9512e-02,  1.5890e-02, -1.1977e-02, -1.0060e-02,\n",
       "                       3.6253e-02,  3.4850e-03,  1.5121e-02, -4.3128e-03,  1.9755e-03,\n",
       "                      -2.9267e-02, -1.9995e-03,  3.3401e-03,  1.2207e-02, -3.0855e-05,\n",
       "                      -4.5240e-03,  2.3982e-02,  1.4829e-02, -2.5526e-02,  3.9774e-02,\n",
       "                       1.1988e-04, -1.6993e-02,  8.6717e-03, -2.4821e-02, -3.8804e-02,\n",
       "                      -1.6521e-02, -9.0235e-03,  2.1312e-02, -7.6228e-03, -2.5650e-02,\n",
       "                       1.4370e-02, -3.6473e-02, -1.2056e-02,  3.2462e-02, -3.4236e-03,\n",
       "                       1.4150e-02, -1.7254e-03, -1.7187e-02,  1.0321e-02, -2.4696e-02,\n",
       "                       6.2996e-03, -2.4094e-03,  5.3147e-03,  2.0040e-02,  1.7210e-03,\n",
       "                       4.4680e-03, -1.9331e-02,  1.8493e-02,  1.1198e-02, -1.6765e-02,\n",
       "                      -5.1137e-03,  1.4068e-02, -2.0710e-02,  3.4963e-03, -2.4113e-02,\n",
       "                      -1.2140e-02, -1.8711e-02, -1.5771e-02,  6.6946e-03,  1.8288e-02,\n",
       "                       2.0705e-02,  3.8506e-02, -1.0912e-02, -2.9057e-03,  1.0413e-02,\n",
       "                      -2.9458e-03, -2.6387e-02, -1.6301e-02, -2.2363e-02,  3.3072e-03,\n",
       "                       6.1849e-03, -1.4873e-03,  1.6906e-02, -3.5344e-02, -3.0207e-03,\n",
       "                      -5.4554e-03,  3.6776e-03, -2.0438e-02, -3.6976e-02, -3.3062e-02,\n",
       "                      -1.3866e-03, -1.4548e-02,  9.2545e-03,  1.1236e-02, -1.2984e-02,\n",
       "                      -8.2824e-03,  2.9620e-03, -8.7627e-03, -1.0037e-02, -8.9495e-03,\n",
       "                      -9.1054e-03,  1.2553e-02,  3.7807e-03, -8.3760e-03, -9.8181e-03,\n",
       "                       2.0377e-02, -3.2537e-02, -1.4104e-02, -1.3852e-02,  3.2071e-02,\n",
       "                      -1.9564e-03,  9.6595e-03, -3.0064e-02,  2.0515e-02, -5.6898e-03,\n",
       "                      -1.0827e-02, -7.7467e-03,  1.8778e-02, -9.1092e-03, -3.4349e-02,\n",
       "                       7.5774e-03, -2.2721e-04, -2.2685e-03,  2.0302e-02,  9.7650e-03,\n",
       "                      -1.5113e-02, -3.6369e-03, -8.9623e-03,  1.6231e-02,  1.6465e-02,\n",
       "                      -1.2799e-02,  2.1387e-03, -7.9900e-03, -2.5105e-02,  2.0544e-03,\n",
       "                       1.8721e-03,  4.1264e-03,  7.2310e-03,  2.7555e-03,  2.0072e-02,\n",
       "                      -3.1577e-02, -5.0546e-03, -1.1574e-02,  5.9842e-03, -9.8609e-03,\n",
       "                       1.2664e-02, -1.3648e-03,  1.1488e-02, -4.4845e-03,  1.2950e-02,\n",
       "                      -1.9456e-02, -1.1047e-03,  1.3133e-02,  5.6618e-04, -8.6550e-03,\n",
       "                       2.2328e-02, -1.1437e-02,  2.6368e-02, -2.7697e-02,  4.3348e-03,\n",
       "                       1.5985e-02, -6.0794e-03,  3.8713e-02,  2.5014e-02,  1.7353e-02,\n",
       "                      -2.9469e-02, -2.4629e-02,  1.5084e-02,  1.5868e-02, -6.3057e-03,\n",
       "                       1.1925e-03, -2.0805e-02, -7.9656e-03,  1.6598e-02, -2.3372e-02,\n",
       "                       1.0827e-02,  1.1367e-02, -6.3750e-03,  2.1685e-02, -3.4450e-02,\n",
       "                       3.1380e-02, -2.5756e-03,  1.7163e-03,  6.0439e-03, -4.6297e-03,\n",
       "                       2.2997e-02,  2.9615e-02,  2.8917e-02,  9.1835e-03, -1.2979e-03,\n",
       "                       3.1183e-02, -1.6812e-02,  2.6709e-02, -1.1760e-02,  2.2019e-02,\n",
       "                      -2.5800e-02,  2.2932e-02,  2.3377e-03, -2.6912e-02,  9.8482e-03,\n",
       "                      -6.3909e-03, -1.7014e-03,  1.3099e-02, -6.3139e-03,  9.5651e-03,\n",
       "                       3.6783e-02,  1.8418e-04,  6.7904e-03, -8.6822e-03, -1.6790e-02,\n",
       "                       3.5112e-02], device='cuda:0')),\n",
       "             ('model.lru_blocks.1.feed_forward.w_2.weight',\n",
       "              tensor([[-0.0087, -0.0220,  0.0352,  ...,  0.0309,  0.0087,  0.0015],\n",
       "                      [-0.0326,  0.0216,  0.0188,  ...,  0.0199, -0.0024, -0.0123],\n",
       "                      [-0.0095, -0.0255,  0.0273,  ..., -0.0357, -0.0075, -0.0005],\n",
       "                      ...,\n",
       "                      [-0.0029, -0.0067,  0.0049,  ..., -0.0275, -0.0031,  0.0175],\n",
       "                      [ 0.0025, -0.0106, -0.0095,  ..., -0.0183,  0.0202, -0.0036],\n",
       "                      [ 0.0193, -0.0033, -0.0079,  ...,  0.0045,  0.0125,  0.0126]],\n",
       "                     device='cuda:0')),\n",
       "             ('model.lru_blocks.1.feed_forward.w_2.bias',\n",
       "              tensor([-0.0355,  0.0029,  0.0182,  0.0269, -0.0165,  0.0045,  0.0046,  0.0075,\n",
       "                      -0.0137, -0.0384, -0.0249, -0.0059,  0.0117, -0.0068, -0.0249, -0.0096,\n",
       "                      -0.0278, -0.0254,  0.0107,  0.0054, -0.0177, -0.0189,  0.0284, -0.0126,\n",
       "                      -0.0327,  0.0247,  0.0026, -0.0120,  0.0122, -0.0006, -0.0116, -0.0192,\n",
       "                       0.0122, -0.0028, -0.0178,  0.0121, -0.0109, -0.0250,  0.0085, -0.0053,\n",
       "                       0.0099,  0.0246,  0.0021,  0.0148,  0.0166, -0.0028, -0.0090, -0.0312,\n",
       "                      -0.0237,  0.0003, -0.0027,  0.0112,  0.0083,  0.0094,  0.0097,  0.0185,\n",
       "                      -0.0113,  0.0117, -0.0164, -0.0256,  0.0142, -0.0252, -0.0168,  0.0053],\n",
       "                     device='cuda:0')),\n",
       "             ('model.lru_blocks.1.feed_forward.layer_norm.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')),\n",
       "             ('model.lru_blocks.1.feed_forward.layer_norm.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "                     device='cuda:0'))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015de891-8a35-4af2-b00d-f2bdcc1085b5",
   "metadata": {},
   "source": [
    "논문 소스에서 저장할때 state_dict 뿐만 아니라 여러 평가 결과를 포함한 dictionary 형태로 저장하기 때문에 아래와 같이 추가적인 접근이 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05f8c75f-5f18-4dfc-bb80-ab8a430bc80a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4113241/1867146058.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(MODEL_PARAMETER_PATH)['model_state_dict'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LRURec(\n",
       "  (embedding): LRUEmbedding(\n",
       "    (token): Embedding(3651, 64)\n",
       "    (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (embed_dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (model): LRUModel(\n",
       "    (lru_blocks): ModuleList(\n",
       "      (0-1): 2 x LRUBlock(\n",
       "        (lru_layer): LRULayer(\n",
       "          (in_proj): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (out_proj): Linear(in_features=128, out_features=64, bias=True)\n",
       "          (out_vector): Identity()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (w_2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (activation): GELU(approximate='none')\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(MODEL_PARAMETER_PATH)['model_state_dict'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6d9675f-ad7f-4913-9464-76908d73a2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LRURec(\n",
      "  (embedding): LRUEmbedding(\n",
      "    (token): Embedding(3651, 64)\n",
      "    (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (embed_dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (model): LRUModel(\n",
      "    (lru_blocks): ModuleList(\n",
      "      (0-1): 2 x LRUBlock(\n",
      "        (lru_layer): LRULayer(\n",
      "          (in_proj): Linear(in_features=64, out_features=128, bias=True)\n",
      "          (out_proj): Linear(in_features=128, out_features=64, bias=True)\n",
      "          (out_vector): Identity()\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "          (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (w_2): Linear(in_features=256, out_features=64, bias=True)\n",
      "          (activation): GELU(approximate='none')\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "          (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b521acb9-5aec-40b4-a97c-7e40e9a46dc0",
   "metadata": {},
   "source": [
    "**신경망 해석**\n",
    "\n",
    "최종 출력: (batch_size, sequence_length, vocab_size(3651)) <br/>\n",
    "윗 구조는 매 시퀸스 위치에서 아이템 3651개 중 어떤 아이템을 추천할지에 대한 점수 -> 모든 후보 아이템에 대한 점수\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf4a33a-7705-4a91-be8e-ffb82ebcb7e5",
   "metadata": {},
   "source": [
    "# Generating Candidate for Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53105b5f-1641-47b2-8d50-2be7a3aead4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seqs shape : torch.Size([64, 50])\n",
      "labels shape : torch.Size([64, 1])\n"
     ]
    }
   ],
   "source": [
    "batch = None\n",
    "seqs = None\n",
    "labels = None\n",
    "\n",
    "for batch in val:\n",
    "    batch = [x.to(device) for x in batch]\n",
    "    seqs, labels = batch\n",
    "    break\n",
    "\n",
    "print(f'seqs shape : {seqs.shape}')\n",
    "print(f'labels shape : {labels.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e434a7-2d36-4e7a-a074-84c7dc80718a",
   "metadata": {},
   "source": [
    "scores[:, -1, :] 하는 이유 -> 추천 모델은 사용자가 최근에 본 아이템(시퀀스의 마지막 위치) 이후에 어떤 아이템을 추천할지 예측하는 데 중점을 둡니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6585c2ac-636b-4614-9e6a-579216e61643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before score shape : torch.Size([64, 50, 3651])\n",
      "score shape : torch.Size([64, 3651])\n"
     ]
    }
   ],
   "source": [
    "scores = model(seqs)\n",
    "print(f\"before score shape : {scores.shape}\")\n",
    "scores = scores[:, -1, :]\n",
    "print(f\"score shape : {scores.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "868d1467-c361-42c9-bcb2-ae6de1caf39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 50\n"
     ]
    }
   ],
   "source": [
    "B, L = seqs.shape\n",
    "print(B, L)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08745d03-e90e-4723-8b81-adbd3cdb8891",
   "metadata": {},
   "source": [
    "**아래 코드 해석**\n",
    "\n",
    "scores에서 3651개의 벡터 값은 현 시퀸스에 대한 모든 아이템 3651개별 점수 값을 의미 <br/>\n",
    "입력으로 집어넣은 seq에 해당 되는 점수값은 이미 사용자가 구매한 아이템이므로 필요하지 않으므로 -1e9값을 넣어 제외함 <br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aba0e951-c87b-44f2-b14c-b10c5366808d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(L):\n",
    "    scores[torch.arange(scores.size(0)), seqs[:, i]] = -1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1857631-1e84-4ae0-9a17-f836baa0e926",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[:, 0] = -1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ca59a44-7884-4926-a6a8-60cacc8a2db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_probs = []\n",
    "val_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1471a587-eb6d-4644-8dec-34316da14639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores shape : (64, 3651)\n",
      "labels shape : (64,)\n"
     ]
    }
   ],
   "source": [
    "val_probs.extend(scores.tolist())\n",
    "val_labels.extend(labels.view(-1).tolist())\n",
    "\n",
    "print(f'scores shape : {np.array(val_probs).shape}')\n",
    "print(f'labels shape : {np.array(val_labels).shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fde8926e-34b9-461a-939a-33b6a0ea94ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1304,    2, 2826, 2752,  247,  778,  109,  116, 1820, 3476, 1318, 1161,\n",
       "        2394,  371, 3030, 1152,   99, 2982, 1227, 1358, 1915, 2539,    6, 3047,\n",
       "        2982,  297, 1634,  145, 3236,   98,  478,  873, 1346, 2155,  467,  667,\n",
       "         509,  312, 1275,  175, 3316, 1733,  780,  478, 2388,  257, 3104,  160,\n",
       "        2927,  897, 2519, 1111,  771,  463, 2423,  307, 1164,  538, 1735,  864,\n",
       "        3490, 3120, 3618, 1796], device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.view(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35956330-2782-44e9-bf21-cc5dbb0bd02e",
   "metadata": {},
   "source": [
    "val_probs -> 64개 유저별 모델이 예측한 점수 값 <br/>\n",
    "val_labels -> 64개 유저별 실제 정답값"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7702fe1e-4cd0-45c1-b86f-f401f6a9256c",
   "metadata": {},
   "source": [
    "# Generating Candidate for Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff38912-4b62-4dcc-9b18-c4448129fb06",
   "metadata": {},
   "source": [
    "validaiont set과 작업 동일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdc59b2-39f9-4589-bb38-77ddb08251b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "llama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
